<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/blog/2022/07/24/hello-world/"/>
      <url>/blog/2022/07/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot+Quartz</title>
      <link href="/blog/2021/02/24/java/0/"/>
      <url>/blog/2021/02/24/java/0/</url>
      
        <content type="html"><![CDATA[<h1 id="目标：实现定时任务的动态创建、启动、暂停、恢复、删除"><a href="#目标：实现定时任务的动态创建、启动、暂停、恢复、删除" class="headerlink" title="目标：实现定时任务的动态创建、启动、暂停、恢复、删除"></a>目标：实现定时任务的动态创建、启动、暂停、恢复、删除</h1><p>看了好多文章，都只讲了基础的demo用法，也就是简单的创建运行定时任务，对定时任务的管理却很少。</p><p>我这里从0开始搭建一个简单的demo，包括定时任务的各种操作，以及API的一些用法，可以实现大多场景的需求。如：</p><ol><li>普通定时任务的创建、启动、停止。</li><li>动态创建定时任务，如创建一个订单，5分钟后执行某某操作。</li></ol><h1 id="整个-Quartz-的代码流程基本基本如下"><a href="#整个-Quartz-的代码流程基本基本如下" class="headerlink" title="整个 Quartz 的代码流程基本基本如下"></a>整个 Quartz 的代码流程基本基本如下</h1><ol><li>首先需要创建我们的任务(Job)，比如取消订单、定时发送短信邮件之类的，这是我们的任务主体，也是写业务逻辑的地方。</li><li>创建任务调度器(Scheduler)，这是用来调度任务的,主要用于启动、停止、暂停、恢复等操作，也就是那几个api的用法。</li><li>创建任务明细(JobDetail)，最开始我们编写好任务(Job)后，只是写好业务代码，并没有触发，这里需要用JobDetail来和之前创建的任务(Job)关联起来，便于执行。</li><li>创建触发器(Trigger)，触发器是来定义任务的规则的，比如几点执行，几点结束，几分钟执行一次等等。这里触发器主要有两大类(SimpleTrigger和CronTrigger)。</li><li>根据Scheduler来启动JobDetail与Trigger</li></ol><h1 id="进入正题，引入依赖"><a href="#进入正题，引入依赖" class="headerlink" title="进入正题，引入依赖"></a><strong>进入正题，引入依赖</strong></h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-quartz<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h1 id="创建Job"><a href="#创建Job" class="headerlink" title="创建Job"></a>创建Job</h1><p>需实现Job接口，这个接口就一个execute()方法需要重写，方法内容就是具体的业务逻辑。如果是动态任务呢，比如取消订单，每次执行都是不同的订单号。</p><p>这个时候就需要在创建任务(JobDetail)或者创建触发器(Trigger)的那里传入参数，然后在这里通过JobExecutionContext来获取参数进行处理，</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.dy.utils.DateUtil;</span><br><span class="line"><span class="keyword">import</span> org.quartz.DisallowConcurrentExecution;</span><br><span class="line"><span class="keyword">import</span> org.quartz.Job;</span><br><span class="line"><span class="keyword">import</span> org.quartz.JobExecutionContext;</span><br><span class="line"><span class="keyword">import</span> org.quartz.JobExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@program</span>: xiudo-ota</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@description</span>: 测试定时任务</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span>: liuhz</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@create</span>: 2020-10-09 14:38</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DisallowConcurrentExecution</span><span class="comment">//Job中的任务有可能并发执行，例如任务的执行时间过长，而每次触发的时间间隔太短，则会导致任务会被并发执行。如果是并发执行，就需要一个数据库锁去避免一个数据被多次处理。</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestJob</span> <span class="keyword">implements</span> <span class="title class_">Job</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">execute</span><span class="params">(JobExecutionContext jobExecutionContext)</span> <span class="keyword">throws</span> JobExecutionException &#123;</span><br><span class="line">        System.err.println(jobExecutionContext.getJobDetail().getJobDataMap().get(<span class="string">&quot;name&quot;</span>));</span><br><span class="line">        System.err.println(jobExecutionContext.getJobDetail().getJobDataMap().get(<span class="string">&quot;age&quot;</span>));</span><br><span class="line">        System.err.println(jobExecutionContext.getTrigger().getJobDataMap().get(<span class="string">&quot;orderNo&quot;</span>));</span><br><span class="line">        System.err.println(<span class="string">&quot;定时任务执行，当前时间：&quot;</span>+ DateUtil.formatDateTime(<span class="keyword">new</span> <span class="title class_">Date</span>()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="创建任务调度器-Scheduler"><a href="#创建任务调度器-Scheduler" class="headerlink" title="创建任务调度器(Scheduler)"></a><strong>创建任务调度器(Scheduler)</strong></h1><p>这里采用Spring IOC，所以直接注入完事。如果是普通的，则需通过工厂创建。</p><p>工厂：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">SchedulerFactory</span> <span class="variable">schedulerFactory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StdSchedulerFactory</span>();</span><br><span class="line"><span class="type">Scheduler</span> <span class="variable">scheduler</span> <span class="operator">=</span> schedulerFactory.getScheduler();</span><br></pre></td></tr></table></figure><p>IOC：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> Scheduler scheduler;</span><br></pre></td></tr></table></figure><h1 id="创建任务明细-JobDetail"><a href="#创建任务明细-JobDetail" class="headerlink" title="创建任务明细(JobDetail)"></a><strong>创建任务明细(JobDetail)</strong></h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**通过JobBuilder.newJob()方法获取到当前Job的具体实现(以下均为链式调用)</span></span><br><span class="line"><span class="comment"> * 这里是固定Job创建，所以代码写死XXX.class</span></span><br><span class="line"><span class="comment"> * 如果是动态的，根据不同的类来创建Job，则 ((Job)Class.forName(&quot;com.zy.job.TestJob&quot;).newInstance()).getClass()</span></span><br><span class="line"><span class="comment"> * 即是 JobBuilder.newJob(((Job)Class.forName(&quot;com.zy.job.TestJob&quot;).newInstance()).getClass())</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="type">JobDetail</span> <span class="variable">jobDetail</span> <span class="operator">=</span> JobBuilder.newJob(TestJob.class)</span><br><span class="line">        <span class="comment">/**给当前JobDetail添加参数，K V形式*/</span></span><br><span class="line">        .usingJobData(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;zy&quot;</span>)</span><br><span class="line">        <span class="comment">/**给当前JobDetail添加参数，K V形式，链式调用，可以传入多个参数，在Job实现类中，可以通过jobExecutionContext.getJobDetail().getJobDataMap().get(&quot;age&quot;)获取值*/</span></span><br><span class="line">        .usingJobData(<span class="string">&quot;age&quot;</span>,<span class="number">23</span>)</span><br><span class="line">        <span class="comment">/**添加认证信息，有3种重写的方法，我这里是其中一种，可以查看源码看其余2种*/</span></span><br><span class="line">        .withIdentity(<span class="string">&quot;我是name&quot;</span>,<span class="string">&quot;我是group&quot;</span>)</span><br><span class="line">        .build();<span class="comment">//执行</span></span><br></pre></td></tr></table></figure><h1 id="创建触发器-Trigger"><a href="#创建触发器-Trigger" class="headerlink" title="创建触发器(Trigger)"></a><strong>创建触发器(Trigger)</strong></h1><p>这里主要分为两大类SimpleTrigger、CronTrigger。</p><p>SimpleTrigger：是根据它自带的api方法设置规则，比如每隔5秒执行一次、每隔1小时执行一次。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Trigger</span> <span class="variable">trigger</span> <span class="operator">=</span> TriggerBuilder.newTrigger()</span><br><span class="line">        <span class="comment">/**给当前JobDetail添加参数，K V形式，链式调用，可以传入多个参数，在Job实现类中，可以通过jobExecutionContext.getTrigger().getJobDataMap().get(&quot;orderNo&quot;)获取值*/</span></span><br><span class="line">        .usingJobData(<span class="string">&quot;orderNo&quot;</span>, <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">        <span class="comment">/**添加认证信息，有3种重写的方法，我这里是其中一种，可以查看源码看其余2种*/</span></span><br><span class="line">        .withIdentity(<span class="string">&quot;我是name&quot;</span>,<span class="string">&quot;我是group&quot;</span>)</span><br><span class="line">        <span class="comment">/**立即生效*/</span></span><br><span class="line"><span class="comment">//      .startNow()</span></span><br><span class="line">        <span class="comment">/**开始执行时间*/</span></span><br><span class="line">        .startAt(start)</span><br><span class="line">        <span class="comment">/**结束执行时间,不写永久执行*/</span></span><br><span class="line">        .endAt(start)</span><br><span class="line">        <span class="comment">/**添加执行规则，SimpleTrigger、CronTrigger的区别主要就在这里*/</span></span><br><span class="line">        .withSchedule(</span><br><span class="line">                SimpleScheduleBuilder.simpleSchedule()</span><br><span class="line">                <span class="comment">/**每隔3s执行一次,api方法有好多规则自行查看*/</span></span><br><span class="line">                .withIntervalInSeconds(<span class="number">3</span>)</span><br><span class="line">                <span class="comment">/**一直执行,如果不写,定时任务就执行一次*/</span></span><br><span class="line">                .repeatForever()</span><br><span class="line">        )</span><br><span class="line">        .build();<span class="comment">//执行</span></span><br></pre></td></tr></table></figure><p>CronTrigger：这就比较常用了，是基于Cron表达式来实现的。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">CronTrigger</span>  <span class="variable">trigger</span> <span class="operator">=</span> TriggerBuilder.newTrigger()</span><br><span class="line">        <span class="comment">/**给当前JobDetail添加参数，K V形式，链式调用，可以传入多个参数，在Job实现类中，可以通过jobExecutionContext.getTrigger().getJobDataMap().get(&quot;orderNo&quot;)获取值*/</span></span><br><span class="line">        .usingJobData(<span class="string">&quot;orderNo&quot;</span>, <span class="string">&quot;123456&quot;</span>)</span><br><span class="line">        <span class="comment">/**添加认证信息，有3种重写的方法，我这里是其中一种，可以查看源码看其余2种*/</span></span><br><span class="line">        .withIdentity(<span class="string">&quot;我是name&quot;</span>,<span class="string">&quot;我是group&quot;</span>)</span><br><span class="line">        <span class="comment">/**立即生效*/</span></span><br><span class="line"><span class="comment">//      .startNow()</span></span><br><span class="line">        <span class="comment">/**开始执行时间*/</span></span><br><span class="line">        .startAt(start)</span><br><span class="line">        <span class="comment">/**结束执行时间,不写永久执行*/</span></span><br><span class="line">        .endAt(start)</span><br><span class="line">        <span class="comment">/**添加执行规则，SimpleTrigger、CronTrigger的区别主要就在这里,我这里是demo，写了个每2分钟执行一次*/</span></span><br><span class="line">        .withSchedule(CronScheduleBuilder.cronSchedule(<span class="string">&quot;0 0/2 * * * ?&quot;</span>))</span><br><span class="line">        .build();<span class="comment">//执行</span></span><br></pre></td></tr></table></figure><p>注意：.startNow( )和.startAt( )这里有个坑，这两个方法是对同一个成员变量进行修改的 也就是说startAt和startNow同时调用的时候任务开始的时间是按后面调用的方法为主的，谁写在后面用谁</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-202002241430000001.png"></p><h1 id="启动任务"><a href="#启动任务" class="headerlink" title="启动任务"></a><strong>启动任务</strong></h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**添加定时任务*/</span></span><br><span class="line">scheduler.scheduleJob(jobDetail, trigger);</span><br><span class="line"><span class="keyword">if</span> (!scheduler.isShutdown()) &#123;</span><br><span class="line">    <span class="comment">/**启动*/</span></span><br><span class="line">    scheduler.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上，任务的创建启动都完事了，后面就是任务的暂停、恢复、删除。比较简单，大致原理就是我们在创建任务明细(JobDetail)和创建触发器(Trigger)时，会调用.withIdentity(key,group)来传入认证信息，后续就是根据这些认证信息来管理任务(通过api方法)</p><h1 id="任务的暂停"><a href="#任务的暂停" class="headerlink" title="任务的暂停"></a><strong>任务的暂停</strong></h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler.pauseTrigger(TriggerKey.triggerKey(<span class="string">&quot;我是刚才写的name&quot;</span>,<span class="string">&quot;我是刚才写的group&quot;</span>));</span><br></pre></td></tr></table></figure><h1 id="任务的恢复"><a href="#任务的恢复" class="headerlink" title="任务的恢复"></a><strong>任务的恢复</strong></h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler.resumeTrigger(TriggerKey.triggerKey(<span class="string">&quot;我是刚才写的name&quot;</span>,<span class="string">&quot;我是刚才写的group&quot;</span>));</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20200224143100002.png"></p><p>根据你写的方式来获取。</p><h1 id="任务的删除"><a href="#任务的删除" class="headerlink" title="任务的删除"></a><strong>任务的删除</strong></h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//暂停触发器</span></span><br><span class="line">scheduler.pauseTrigger(TriggerKey.triggerKey(<span class="string">&quot;我是刚才写的name&quot;</span>,<span class="string">&quot;我是刚才写的group&quot;</span>));</span><br><span class="line"><span class="comment">//移除触发器</span></span><br><span class="line">scheduler.unscheduleJob(TriggerKey.triggerKey(<span class="string">&quot;我是刚才写的name&quot;</span>,<span class="string">&quot;我是刚才写的group&quot;</span>));</span><br><span class="line"><span class="comment">//删除Job</span></span><br><span class="line">scheduler.deleteJob(JobKey.jobKey(<span class="string">&quot;我是刚才写的name&quot;</span>,<span class="string">&quot;我是刚才写的group&quot;</span>));</span><br></pre></td></tr></table></figure><p>最后附上基本代码，Job实现在上面：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> Scheduler scheduler;</span><br><span class="line"> </span><br><span class="line"><span class="meta">@PostMapping(&quot;/Quartz&quot;)</span></span><br><span class="line"><span class="meta">@ApiOperation(value = &quot;定时任务_创建&quot;, notes = &quot;创建&quot;)</span></span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">quartz</span><span class="params">(<span class="meta">@RequestParam(&quot;orderNo&quot;)</span>  String orderNo)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    Date start=<span class="keyword">new</span> <span class="title class_">Date</span>(System.currentTimeMillis() + <span class="number">7</span> * <span class="number">1000</span>);<span class="comment">//当前时间7秒之后</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**通过JobBuilder.newJob()方法获取到当前Job的具体实现(以下均为链式调用)</span></span><br><span class="line"><span class="comment">     * 这里是固定Job创建，所以代码写死XXX.class</span></span><br><span class="line"><span class="comment">     * 如果是动态的，根据不同的类来创建Job，则((Job)Class.forName(&quot;com.zy.job.TestJob&quot;).newInstance()).getClass()</span></span><br><span class="line"><span class="comment">     * 即是 JobBuilder.newJob(((Job)Class.forName(&quot;com.zy.job.TestJob&quot;).newInstance()).getClass())</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="type">JobDetail</span> <span class="variable">jobDetail</span> <span class="operator">=</span> JobBuilder.newJob(TestJob.class)</span><br><span class="line">        <span class="comment">/**给当前JobDetail添加参数，K V形式*/</span></span><br><span class="line">        .usingJobData(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;zy&quot;</span>)</span><br><span class="line">        <span class="comment">/**给当前JobDetail添加参数，K V形式，链式调用，可以传入多个参数，在Job实现类中，可以通过</span></span><br><span class="line"><span class="comment">        jobExecutionContext.getJobDetail().getJobDataMap().get(&quot;age&quot;)获取值*/</span></span><br><span class="line">        .usingJobData(<span class="string">&quot;age&quot;</span>,<span class="number">23</span>)</span><br><span class="line">        <span class="comment">/**添加认证信息，有3种重写的方法，我这里是其中一种，可以查看源码看其余2种*/</span></span><br><span class="line">        .withIdentity(orderNo)</span><br><span class="line">        .build();<span class="comment">//执行</span></span><br><span class="line"></span><br><span class="line">    <span class="type">Trigger</span> <span class="variable">trigger</span> <span class="operator">=</span> TriggerBuilder.newTrigger()</span><br><span class="line">        <span class="comment">/**给当前JobDetail添加参数，K V形式，链式调用，可以传入多个参数，在Job实现类中，可以通过</span></span><br><span class="line"><span class="comment">        jobExecutionContext.getTrigger().getJobDataMap().get(&quot;orderNo&quot;)获取值*/</span></span><br><span class="line">        .usingJobData(<span class="string">&quot;orderNo&quot;</span>, orderNo)</span><br><span class="line">        <span class="comment">/**添加认证信息，有3种重写的方法，我这里是其中一种，可以查看源码看其余2种*/</span></span><br><span class="line">        .withIdentity(orderNo)</span><br><span class="line">        <span class="comment">/**立即生效*/</span></span><br><span class="line">        <span class="comment">//      .startNow()</span></span><br><span class="line">        <span class="comment">/**开始执行时间*/</span></span><br><span class="line">        .startAt(start)</span><br><span class="line">        <span class="comment">/**结束执行时间*/</span></span><br><span class="line">        <span class="comment">//        .endAt(start)</span></span><br><span class="line">        <span class="comment">/**添加执行规则，SimpleTrigger、CronTrigger的区别主要就在这里*/</span></span><br><span class="line">        .withSchedule(</span><br><span class="line">        SimpleScheduleBuilder.simpleSchedule()</span><br><span class="line">        <span class="comment">/**每隔1s执行一次*/</span></span><br><span class="line">        .withIntervalInSeconds(<span class="number">3</span>)</span><br><span class="line">        <span class="comment">/**一直执行，*/</span></span><br><span class="line">        .repeatForever()</span><br><span class="line">    )</span><br><span class="line">    .build();<span class="comment">//执行</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//CronTrigger  trigger = TriggerBuilder.newTrigger()</span></span><br><span class="line"><span class="comment">//        /**给当前JobDetail添加参数，K V形式，链式调用，可以传入多个参数，在Job实现类中，可以通过jobExecutionContext.getTrigger().getJobDataMap().get(&quot;orderNo&quot;)获取值*/</span></span><br><span class="line"><span class="comment">//        .usingJobData(&quot;orderNo&quot;, orderNo)</span></span><br><span class="line"><span class="comment">//        /**添加认证信息，有3种重写的方法，我这里是其中一种，可以查看源码看其余2种*/</span></span><br><span class="line"><span class="comment">//        .withIdentity(orderNo)</span></span><br><span class="line"><span class="comment">//        /**开始执行时间*/</span></span><br><span class="line"><span class="comment">//        .startAt(start)</span></span><br><span class="line"><span class="comment">//        /**结束执行时间*/</span></span><br><span class="line"><span class="comment">//        .endAt(start)</span></span><br><span class="line"><span class="comment">//        /**添加执行规则，SimpleTrigger、CronTrigger的区别主要就在这里*/</span></span><br><span class="line"><span class="comment">//        .withSchedule(CronScheduleBuilder.cronSchedule(&quot;* 30 10 ? * 1/5 2018&quot;))</span></span><br><span class="line"><span class="comment">//        .build();//执行</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="comment">/**添加定时任务*/</span></span><br><span class="line">    scheduler.scheduleJob(jobDetail, trigger);</span><br><span class="line">    <span class="keyword">if</span> (!scheduler.isShutdown()) &#123;</span><br><span class="line">        <span class="comment">/**启动*/</span></span><br><span class="line">        scheduler.start();</span><br><span class="line">    &#125;</span><br><span class="line">    System.err.println(<span class="string">&quot;--------定时任务启动成功 &quot;</span>+<span class="keyword">new</span> <span class="title class_">SimpleDateFormat</span>(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>).format(<span class="keyword">new</span> <span class="title class_">Date</span>())+<span class="string">&quot; ------------&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;ok&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@PostMapping(&quot;/shutdown&quot;)</span></span><br><span class="line"><span class="meta">@ApiOperation(value = &quot;定时任务_停止&quot;, notes = &quot;停止&quot;)</span></span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">shutdown</span><span class="params">(<span class="meta">@RequestParam(&quot;orderNo&quot;)</span>  String orderNo)</span> <span class="keyword">throws</span> IOException, SchedulerException &#123;</span><br><span class="line">    scheduler.pauseTrigger(TriggerKey.triggerKey(orderNo));<span class="comment">//暂停Trigger</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@PostMapping(&quot;/resume&quot;)</span></span><br><span class="line"><span class="meta">@ApiOperation(value = &quot;定时任务_恢复&quot;, notes = &quot;恢复&quot;)</span></span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">resume</span><span class="params">(<span class="meta">@RequestParam(&quot;orderNo&quot;)</span>  String orderNo)</span> <span class="keyword">throws</span> IOException, SchedulerException &#123;</span><br><span class="line">    scheduler.resumeTrigger(TriggerKey.triggerKey(orderNo));<span class="comment">//恢复Trigger</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;ok&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@PostMapping(&quot;/del&quot;)</span></span><br><span class="line"><span class="meta">@ApiOperation(value = &quot;定时任务_删除&quot;, notes = &quot;删除&quot;)</span></span><br><span class="line"><span class="meta">@ResponseBody</span></span><br><span class="line"><span class="keyword">public</span> Object <span class="title function_">del</span><span class="params">(<span class="meta">@RequestParam(&quot;orderNo&quot;)</span>  String orderNo)</span> <span class="keyword">throws</span> IOException, SchedulerException &#123;</span><br><span class="line">    scheduler.pauseTrigger(TriggerKey.triggerKey(orderNo));<span class="comment">//暂停触发器</span></span><br><span class="line">    scheduler.unscheduleJob(TriggerKey.triggerKey(orderNo));<span class="comment">//移除触发器</span></span><br><span class="line">    scheduler.deleteJob(JobKey.jobKey(orderNo));<span class="comment">//删除Job</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;ok&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完事。如果想让定时任务在启动项目后自动启动，则需要持久化任务，可以把基本信息保存在数据库，项目启动时启动完，或者做分布式任务</p>]]></content>
      
      
      <categories>
          
          <category> SpringBoot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SpringBoot </tag>
            
            <tag> Quartz </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Boot+Prometheus+Grafana</title>
      <link href="/blog/2021/02/24/prometheus/0/"/>
      <url>/blog/2021/02/24/prometheus/0/</url>
      
        <content type="html"><![CDATA[<p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400001.png"></p><h1 id="Spring-Boot-应用暴露监控指标【版本-1-5-7-RELEASE】"><a href="#Spring-Boot-应用暴露监控指标【版本-1-5-7-RELEASE】" class="headerlink" title="Spring Boot 应用暴露监控指标【版本 1.5.7.RELEASE】"></a><strong>Spring Boot 应用暴露监控指标【版本 1.5.7.RELEASE】</strong></h1><p>首先，添加依赖如下依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-actuator<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>采集应用的指标信息，我们使用的是prometheus,相应的我们引入包:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.prometheus<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>simpleclient_spring_boot<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>0.0.26<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后，在启动类 Application.java 添加如下注解：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@EnablePrometheusEndpoint</span></span><br><span class="line"><span class="meta">@EnableSpringBootMetricsCollector</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Application</span> &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    SpringApplication.run(Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后，配置默认的登录账号和密码，在 application.yml 中：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">security:</span> </span><br><span class="line"><span class="attr">user:</span> </span><br><span class="line"><span class="attr">name:</span> <span class="string">user</span> </span><br><span class="line"><span class="attr">password:</span> <span class="string">pwd</span> </span><br></pre></td></tr></table></figure><p>启动应用程序后，会看到如下一系列的 Mappings</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400002.jpg"></p><p>利用账号密码访问 <a href="http://localhost:8080/application/prometheus">http://localhost:8080/application/prometheus</a> ，可以看到 Prometheus 格式的指标数据<br><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400003.jpg"></p><h1 id="Prometheus-采集-Spring-Boot-指标数据"><a href="#Prometheus-采集-Spring-Boot-指标数据" class="headerlink" title="Prometheus 采集 Spring Boot 指标数据"></a><strong>Prometheus 采集 Spring Boot 指标数据</strong></h1><p>首先，获取 Prometheus 的 Docker 镜像：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull prom/prometheus</span><br></pre></td></tr></table></figure><p>然后，编写配置文件 prometheus.yml ：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">global:</span></span><br><span class="line">  <span class="attr">scrape_interval:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">scrape_timeout:</span> <span class="string">10s</span></span><br><span class="line">  <span class="attr">evaluation_interval:</span> <span class="string">10m</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">spring-boot</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">scrape_timeout:</span> <span class="string">5s</span></span><br><span class="line">    <span class="attr">metrics_path:</span> <span class="string">/application/prometheus</span></span><br><span class="line">    <span class="attr">scheme:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">basic_auth:</span></span><br><span class="line">      <span class="attr">username:</span> <span class="string">admin</span></span><br><span class="line">      <span class="attr">password:</span> <span class="number">123456</span></span><br><span class="line">    <span class="attr">static_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">targets:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.11</span><span class="number">.54</span><span class="string">:8099</span> <span class="comment">#此处填写 Spring Boot 应用的 IP + 端口号</span></span><br></pre></td></tr></table></figure><p>接着，启动 Prometheus :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name prometheus -p 9090:9090 -v prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus</span><br></pre></td></tr></table></figure><blockquote><p>请注意，D:\test\actuator\prometheus\prometheus.yml ，是我的配置文件存放地址，我们需要将它放到容器里面去，所以用了-v来做文件映射。&#x2F;etc&#x2F;prometheus&#x2F;prometheus.yml这个是容器启动的时候去取的默认配置，这里我是直接覆盖掉了它。prom&#x2F;prometheus这是镜像，如果本地没有，就回去你设置好的镜像仓库去取。</p></blockquote><p>启动完成后用docker ps看下是否已经启动成功，之后打开浏览器输入：<br><a href="http://localhost:9090/targets,%E6%A3%80%E6%9F%A5">http://localhost:9090/targets,检查</a> Spring Boot 采集状态是否正常,如果看到下图就是成功了。</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400004.jpg"></p><h1 id="Grafana-可视化监控数据"><a href="#Grafana-可视化监控数据" class="headerlink" title="Grafana 可视化监控数据"></a><strong>Grafana 可视化监控数据</strong></h1><p>首先，获取 Grafana 的 Docker 镜像：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull grafana/grafana1</span><br></pre></td></tr></table></figure><p>然后，启动 Grafana：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --name grafana -d -p 3000:3000 grafana/grafana1</span><br></pre></td></tr></table></figure><p>接着，访问 <a href="http://localhost:3000/">http://localhost:3000/</a> 配置 Prometheus 数据源：</p><blockquote><p>Grafana 登录账号 admin 密码 admin</p></blockquote><ol><li>先配置数据源.</li></ol><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400005.png"></p><p>2.配置单个指标的可视化监控面板：</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400006.jpg"></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400007.jpg"></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400008.jpg"></p><p>prometh采集的数据</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400009.png"></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210224115400010.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> grafana </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes搭建高可用集群</title>
      <link href="/blog/2021/02/01/kubenetes/17/"/>
      <url>/blog/2021/02/01/kubenetes/17/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes搭建高可用集群"><a href="#Kubernetes搭建高可用集群" class="headerlink" title="Kubernetes搭建高可用集群"></a>Kubernetes搭建高可用集群</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前我们搭建的集群，只有一个master节点，当master节点宕机的时候，通过node将无法继续访问，而master主要是管理作用，所以整个集群将无法提供服务</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121164522945.png"></p><h2 id="高可用集群"><a href="#高可用集群" class="headerlink" title="高可用集群"></a>高可用集群</h2><p>下面我们就需要搭建一个多master节点的高可用集群，不会存在单点故障问题</p><p>但是在node 和 master节点之间，需要存在一个 LoadBalancer组件，作用如下：</p><ul><li>负载</li><li>检查master节点的状态</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121164931760.png"></p><p>对外有一个统一的VIP：虚拟ip来对外进行访问</p><h2 id="高可用集群技术细节"><a href="#高可用集群技术细节" class="headerlink" title="高可用集群技术细节"></a>高可用集群技术细节</h2><p>高可用集群技术细节如下所示：</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121165325194.png"></p><ul><li>keepalived：配置虚拟ip，检查节点的状态</li><li>haproxy：负载均衡服务【类似于nginx】</li><li>apiserver：</li><li>controller：</li><li>manager：</li><li>scheduler：</li></ul><h2 id="高可用集群步骤"><a href="#高可用集群步骤" class="headerlink" title="高可用集群步骤"></a>高可用集群步骤</h2><p>我们采用2个master节点，一个node节点来搭建高可用集群，下面给出了每个节点需要做的事情</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121170351461.png"></p><h2 id="初始化操作"><a href="#初始化操作" class="headerlink" title="初始化操作"></a>初始化操作</h2><p>我们需要在这三个节点上进行操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭防火墙</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭selinux</span></span><br><span class="line"><span class="comment"># 永久关闭</span></span><br><span class="line">sed -i <span class="string">&#x27;s/enforcing/disabled/&#x27;</span> /etc/selinux/config  </span><br><span class="line"><span class="comment"># 临时关闭</span></span><br><span class="line">setenforce 0  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭swap</span></span><br><span class="line"><span class="comment"># 临时</span></span><br><span class="line">swapoff -a </span><br><span class="line"><span class="comment"># 永久关闭</span></span><br><span class="line">sed -ri <span class="string">&#x27;s/.*swap.*/#&amp;/&#x27;</span> /etc/fstab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据规划设置主机名【master1节点上操作】</span></span><br><span class="line">hostnamectl set-hostname master1</span><br><span class="line"><span class="comment"># 根据规划设置主机名【master2节点上操作】</span></span><br><span class="line">hostnamectl set-hostname master1</span><br><span class="line"><span class="comment"># 根据规划设置主机名【node1节点操作】</span></span><br><span class="line">hostnamectl set-hostname node1</span><br><span class="line"></span><br><span class="line"><span class="comment"># r添加hosts</span></span><br><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/hosts &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">192.168.44.158  k8smaster</span></span><br><span class="line"><span class="string">192.168.44.155 master01.k8s.io master1</span></span><br><span class="line"><span class="string">192.168.44.156 master02.k8s.io master2</span></span><br><span class="line"><span class="string">192.168.44.157 node01.k8s.io node1</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将桥接的IPv4流量传递到iptables的链【3个节点上都执行】</span></span><br><span class="line"><span class="built_in">cat</span> &gt; /etc/sysctl.d/k8s.conf &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-ip6tables = 1</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-iptables = 1</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生效</span></span><br><span class="line">sysctl --system  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 时间同步</span></span><br><span class="line">yum install ntpdate -y</span><br><span class="line">ntpdate time.windows.com</span><br></pre></td></tr></table></figure><h2 id="部署keepAlived"><a href="#部署keepAlived" class="headerlink" title="部署keepAlived"></a>部署keepAlived</h2><p>下面我们需要在所有的master节点【master1和master2】上部署keepAlive</p><h3 id="安装相关包"><a href="#安装相关包" class="headerlink" title="安装相关包"></a>安装相关包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装相关工具</span></span><br><span class="line">yum install -y conntrack-tools libseccomp libtool-ltdl</span><br><span class="line"><span class="comment"># 安装keepalived</span></span><br><span class="line">yum install -y keepalived</span><br></pre></td></tr></table></figure><h3 id="配置master节点"><a href="#配置master节点" class="headerlink" title="配置master节点"></a>配置master节点</h3><p>添加master1的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/keepalived/keepalived.conf &lt;&lt;<span class="string">EOF </span></span><br><span class="line"><span class="string">! Configuration File for keepalived</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">global_defs &#123;</span></span><br><span class="line"><span class="string">   router_id k8s</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_script check_haproxy &#123;</span></span><br><span class="line"><span class="string">    script &quot;killall -0 haproxy&quot;</span></span><br><span class="line"><span class="string">    interval 3</span></span><br><span class="line"><span class="string">    weight -2</span></span><br><span class="line"><span class="string">    fall 10</span></span><br><span class="line"><span class="string">    rise 2</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_instance VI_1 &#123;</span></span><br><span class="line"><span class="string">    state MASTER </span></span><br><span class="line"><span class="string">    interface ens33 </span></span><br><span class="line"><span class="string">    virtual_router_id 51</span></span><br><span class="line"><span class="string">    priority 250</span></span><br><span class="line"><span class="string">    advert_int 1</span></span><br><span class="line"><span class="string">    authentication &#123;</span></span><br><span class="line"><span class="string">        auth_type PASS</span></span><br><span class="line"><span class="string">        auth_pass ceb1b3ec013d66163d6ab</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    virtual_ipaddress &#123;</span></span><br><span class="line"><span class="string">        192.168.44.158</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    track_script &#123;</span></span><br><span class="line"><span class="string">        check_haproxy</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>添加master2的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/keepalived/keepalived.conf &lt;&lt;<span class="string">EOF </span></span><br><span class="line"><span class="string">! Configuration File for keepalived</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">global_defs &#123;</span></span><br><span class="line"><span class="string">   router_id k8s</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_script check_haproxy &#123;</span></span><br><span class="line"><span class="string">    script &quot;killall -0 haproxy&quot;</span></span><br><span class="line"><span class="string">    interval 3</span></span><br><span class="line"><span class="string">    weight -2</span></span><br><span class="line"><span class="string">    fall 10</span></span><br><span class="line"><span class="string">    rise 2</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">vrrp_instance VI_1 &#123;</span></span><br><span class="line"><span class="string">    state BACKUP </span></span><br><span class="line"><span class="string">    interface ens33 </span></span><br><span class="line"><span class="string">    virtual_router_id 51</span></span><br><span class="line"><span class="string">    priority 200</span></span><br><span class="line"><span class="string">    advert_int 1</span></span><br><span class="line"><span class="string">    authentication &#123;</span></span><br><span class="line"><span class="string">        auth_type PASS</span></span><br><span class="line"><span class="string">        auth_pass ceb1b3ec013d66163d6ab</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    virtual_ipaddress &#123;</span></span><br><span class="line"><span class="string">        192.168.44.158</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    track_script &#123;</span></span><br><span class="line"><span class="string">        check_haproxy</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h3 id="启动和检查"><a href="#启动和检查" class="headerlink" title="启动和检查"></a>启动和检查</h3><p>在两台master节点都执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动keepalived</span></span><br><span class="line">systemctl start keepalived.service</span><br><span class="line"><span class="comment"># 设置开机启动</span></span><br><span class="line">systemctl <span class="built_in">enable</span> keepalived.service</span><br><span class="line"><span class="comment"># 查看启动状态</span></span><br><span class="line">systemctl status keepalived.service</span><br></pre></td></tr></table></figure><p>启动后查看master的网卡信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip a s ens33</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121171619497.png"></p><h2 id="部署haproxy"><a href="#部署haproxy" class="headerlink" title="部署haproxy"></a>部署haproxy</h2><p>haproxy主要做负载的作用，将我们的请求分担到不同的node节点上</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在两个master节点安装 haproxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装haproxy</span></span><br><span class="line">yum install -y haproxy</span><br><span class="line"><span class="comment"># 启动 haproxy</span></span><br><span class="line">systemctl start haproxy</span><br><span class="line"><span class="comment"># 开启自启</span></span><br><span class="line">systemctl <span class="built_in">enable</span> haproxy</span><br></pre></td></tr></table></figure><p>启动后，我们查看对应的端口是否包含 16443</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -tunlp | grep haproxy</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121181803128.png"></p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>两台master节点的配置均相同，配置中声明了后端代理的两个master节点服务器，指定了haproxy运行的端口为16443等，因此16443端口为集群的入口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/haproxy/haproxy.cfg &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># Global settings</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string">global</span></span><br><span class="line"><span class="string">    # to have these messages end up in /var/log/haproxy.log you will</span></span><br><span class="line"><span class="string">    # need to:</span></span><br><span class="line"><span class="string">    # 1) configure syslog to accept network log events.  This is done</span></span><br><span class="line"><span class="string">    #    by adding the &#x27;-r&#x27; option to the SYSLOGD_OPTIONS in</span></span><br><span class="line"><span class="string">    #    /etc/sysconfig/syslog</span></span><br><span class="line"><span class="string">    # 2) configure local2 events to go to the /var/log/haproxy.log</span></span><br><span class="line"><span class="string">    #   file. A line like the following can be added to</span></span><br><span class="line"><span class="string">    #   /etc/sysconfig/syslog</span></span><br><span class="line"><span class="string">    #</span></span><br><span class="line"><span class="string">    #    local2.*                       /var/log/haproxy.log</span></span><br><span class="line"><span class="string">    #</span></span><br><span class="line"><span class="string">    log         127.0.0.1 local2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    chroot      /var/lib/haproxy</span></span><br><span class="line"><span class="string">    pidfile     /var/run/haproxy.pid</span></span><br><span class="line"><span class="string">    maxconn     4000</span></span><br><span class="line"><span class="string">    user        haproxy</span></span><br><span class="line"><span class="string">    group       haproxy</span></span><br><span class="line"><span class="string">    daemon </span></span><br><span class="line"><span class="string">       </span></span><br><span class="line"><span class="string">    # turn on stats unix socket</span></span><br><span class="line"><span class="string">    stats socket /var/lib/haproxy/stats</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># common defaults that all the &#x27;listen&#x27; and &#x27;backend&#x27; sections will</span></span><br><span class="line"><span class="string"># use if not designated in their block</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------  </span></span><br><span class="line"><span class="string">defaults</span></span><br><span class="line"><span class="string">    mode                    http</span></span><br><span class="line"><span class="string">    log                     global</span></span><br><span class="line"><span class="string">    option                  httplog</span></span><br><span class="line"><span class="string">    option                  dontlognull</span></span><br><span class="line"><span class="string">    option http-server-close</span></span><br><span class="line"><span class="string">    option forwardfor       except 127.0.0.0/8</span></span><br><span class="line"><span class="string">    option                  redispatch</span></span><br><span class="line"><span class="string">    retries                 3</span></span><br><span class="line"><span class="string">    timeout http-request    10s</span></span><br><span class="line"><span class="string">    timeout queue           1m</span></span><br><span class="line"><span class="string">    timeout connect         10s</span></span><br><span class="line"><span class="string">    timeout client          1m</span></span><br><span class="line"><span class="string">    timeout server          1m</span></span><br><span class="line"><span class="string">    timeout http-keep-alive 10s</span></span><br><span class="line"><span class="string">    timeout check           10s</span></span><br><span class="line"><span class="string">    maxconn                 3000</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># kubernetes apiserver frontend which proxys to the backends</span></span><br><span class="line"><span class="string">#--------------------------------------------------------------------- </span></span><br><span class="line"><span class="string">frontend kubernetes-apiserver</span></span><br><span class="line"><span class="string">    mode                 tcp</span></span><br><span class="line"><span class="string">    bind                 *:16443</span></span><br><span class="line"><span class="string">    option               tcplog</span></span><br><span class="line"><span class="string">    default_backend      kubernetes-apiserver    </span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># round robin balancing between the various backends</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string">backend kubernetes-apiserver</span></span><br><span class="line"><span class="string">    mode        tcp</span></span><br><span class="line"><span class="string">    balance     roundrobin</span></span><br><span class="line"><span class="string">    server      master01.k8s.io   192.168.44.155:6443 check</span></span><br><span class="line"><span class="string">    server      master02.k8s.io   192.168.44.156:6443 check</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string"># collection haproxy statistics message</span></span><br><span class="line"><span class="string">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="string">listen stats</span></span><br><span class="line"><span class="string">    bind                 *:1080</span></span><br><span class="line"><span class="string">    stats auth           admin:awesomePassword</span></span><br><span class="line"><span class="string">    stats refresh        5s</span></span><br><span class="line"><span class="string">    stats realm          HAProxy\ Statistics</span></span><br><span class="line"><span class="string">    stats uri            /admin?stats</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h2 id="安装Docker、Kubeadm、kubectl"><a href="#安装Docker、Kubeadm、kubectl" class="headerlink" title="安装Docker、Kubeadm、kubectl"></a>安装Docker、Kubeadm、kubectl</h2><p>所有节点安装Docker&#x2F;kubeadm&#x2F;kubelet ，Kubernetes默认CRI（容器运行时）为Docker，因此先安装Docker</p><h3 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h3><p>首先配置一下Docker的阿里yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;/etc/yum.repos.d/docker.repo&lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">[docker-ce-edge]</span></span><br><span class="line"><span class="string">name=Docker CE Edge - \$basearch</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/\$basearch/edge</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=1</span></span><br><span class="line"><span class="string">gpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后yum方式安装docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum安装</span></span><br><span class="line">yum -y install docker-ce</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看docker版本</span></span><br><span class="line">docker --version  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动docker</span></span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><p>配置docker的镜像源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt;&gt; /etc/docker/daemon.json &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">  &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><p>然后重启docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="添加kubernetes软件源"><a href="#添加kubernetes软件源" class="headerlink" title="添加kubernetes软件源"></a>添加kubernetes软件源</h3><p>然后我们还需要配置一下yum的k8s软件源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">[kubernetes]</span></span><br><span class="line"><span class="string">name=Kubernetes</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=0</span></span><br><span class="line"><span class="string">repo_gpgcheck=0</span></span><br><span class="line"><span class="string">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><h3 id="安装kubeadm，kubelet和kubectl"><a href="#安装kubeadm，kubelet和kubectl" class="headerlink" title="安装kubeadm，kubelet和kubectl"></a>安装kubeadm，kubelet和kubectl</h3><p>由于版本更新频繁，这里指定版本号部署：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装kubelet、kubeadm、kubectl，同时指定版本</span></span><br><span class="line">yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0</span><br><span class="line"><span class="comment"># 设置开机启动</span></span><br><span class="line">systemctl <span class="built_in">enable</span> kubelet</span><br></pre></td></tr></table></figure><h2 id="部署Kubernetes-Master【master节点】"><a href="#部署Kubernetes-Master【master节点】" class="headerlink" title="部署Kubernetes Master【master节点】"></a>部署Kubernetes Master【master节点】</h2><h3 id="创建kubeadm配置文件"><a href="#创建kubeadm配置文件" class="headerlink" title="创建kubeadm配置文件"></a>创建kubeadm配置文件</h3><p>在具有vip的master上进行初始化操作，这里为master1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> /usr/local/kubernetes/manifests -p</span><br><span class="line"><span class="comment"># 到manifests目录</span></span><br><span class="line"><span class="built_in">cd</span> /usr/local/kubernetes/manifests/</span><br><span class="line"><span class="comment"># 新建yaml文件</span></span><br><span class="line">vi kubeadm-config.yaml</span><br></pre></td></tr></table></figure><p>yaml内容如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">apiServer:</span><br><span class="line">  certSANs:</span><br><span class="line">    - master1</span><br><span class="line">    - master2</span><br><span class="line">    - master.k8s.io</span><br><span class="line">    - 192.168.44.158</span><br><span class="line">    - 192.168.44.155</span><br><span class="line">    - 192.168.44.156</span><br><span class="line">    - 127.0.0.1</span><br><span class="line">  extraArgs:</span><br><span class="line">    authorization-mode: Node,RBAC</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controlPlaneEndpoint: <span class="string">&quot;master.k8s.io:16443&quot;</span></span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: </span><br><span class="line">  <span class="built_in">type</span>: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:    </span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.aliyuncs.com/google_containers</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.16.3</span><br><span class="line">networking: </span><br><span class="line">  dnsDomain: cluster.local  </span><br><span class="line">  podSubnet: 10.244.0.0/16</span><br><span class="line">  serviceSubnet: 10.1.0.0/16</span><br><span class="line">scheduler: &#123;&#125;</span><br></pre></td></tr></table></figure><p>然后我们在 master1 节点执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><p>执行完成后，就会在拉取我们的进行了【需要等待…】</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121194928988.png"></p><p>按照提示配置环境变量，使用kubectl工具</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行下方命令</span></span><br><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"><span class="comment"># 查看节点</span></span><br><span class="line">kubectl get nodes</span><br><span class="line"><span class="comment"># 查看pod</span></span><br><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><p><strong>按照提示保存以下内容，一会要使用：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm <span class="built_in">join</span> master.k8s.io:16443 --token jv5z7n.3y1zi95p952y9p65 \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:403bca185c2f3a4791685013499e7ce58f9848e2213e27194b75a2e3293d8812 \</span><br><span class="line">    --control-plane </span><br></pre></td></tr></table></figure><blockquote><p>–control-plane ： 只有在添加master节点的时候才有</p></blockquote><p>查看集群状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看集群状态</span></span><br><span class="line">kubectl get cs</span><br><span class="line"><span class="comment"># 查看pod</span></span><br><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><h2 id="安装集群网络"><a href="#安装集群网络" class="headerlink" title="安装集群网络"></a>安装集群网络</h2><p>从官方地址获取到flannel的yaml，在master1上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> flannel</span><br><span class="line"><span class="built_in">cd</span> flannel</span><br><span class="line"><span class="comment"># 下载yaml文件</span></span><br><span class="line">wget -c https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><p>安装flannel网络</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f kube-flannel.yml </span><br></pre></td></tr></table></figure><p>检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><h2 id="master2节点加入集群"><a href="#master2节点加入集群" class="headerlink" title="master2节点加入集群"></a>master2节点加入集群</h2><h3 id="复制密钥及相关文件"><a href="#复制密钥及相关文件" class="headerlink" title="复制密钥及相关文件"></a>复制密钥及相关文件</h3><p>从master1复制密钥及相关文件到master2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ssh root@192.168.44.156 mkdir -p /etc/kubernetes/pki/etcd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scp /etc/kubernetes/admin.conf root@192.168.44.156:/etc/kubernetes</span></span><br><span class="line">   </span><br><span class="line"><span class="comment"># scp /etc/kubernetes/pki/&#123;ca.*,sa.*,front-proxy-ca.*&#125; root@192.168.44.156:/etc/kubernetes/pki</span></span><br><span class="line">   </span><br><span class="line"><span class="comment"># scp /etc/kubernetes/pki/etcd/ca.* root@192.168.44.156:/etc/kubernetes/pki/etcd</span></span><br></pre></td></tr></table></figure><h3 id="master2加入集群"><a href="#master2加入集群" class="headerlink" title="master2加入集群"></a>master2加入集群</h3><p>执行在master1上init后输出的join命令,需要带上参数<code>--control-plane</code>表示把master控制节点加入集群</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm <span class="built_in">join</span> master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b     --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba --control-plane</span><br></pre></td></tr></table></figure><p>检查状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line"></span><br><span class="line">kubectl get pods --all-namespaces</span><br></pre></td></tr></table></figure><h2 id="加入Kubernetes-Node"><a href="#加入Kubernetes-Node" class="headerlink" title="加入Kubernetes Node"></a>加入Kubernetes Node</h2><p>在node1上执行</p><p>向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm <span class="built_in">join</span> master.k8s.io:16443 --token ckf7bs.30576l0okocepg8b     --discovery-token-ca-cert-hash sha256:19afac8b11182f61073e254fb57b9f19ab4d798b70501036fc69ebef46094aba</span><br></pre></td></tr></table></figure><p><strong>集群网络重新安装，因为添加了新的node节点</strong></p><p>检查状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get node</span><br><span class="line">kubectl get pods --all-namespaces</span><br></pre></td></tr></table></figure><h2 id="测试kubernetes集群"><a href="#测试kubernetes集群" class="headerlink" title="测试kubernetes集群"></a>测试kubernetes集群</h2><p>在Kubernetes集群中创建一个pod，验证是否正常运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建nginx deployment</span></span><br><span class="line">kubectl create deployment nginx --image=nginx</span><br><span class="line"><span class="comment"># 暴露端口</span></span><br><span class="line">kubectl expose deployment nginx --port=80 --<span class="built_in">type</span>=NodePort</span><br><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">kubectl get pod,svc</span><br></pre></td></tr></table></figure><p>然后我们通过任何一个节点，都能够访问我们的nginx页面</p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes集群资源监控</title>
      <link href="/blog/2021/02/01/kubenetes/16/"/>
      <url>/blog/2021/02/01/kubenetes/16/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes集群资源监控"><a href="#Kubernetes集群资源监控" class="headerlink" title="Kubernetes集群资源监控"></a>Kubernetes集群资源监控</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="监控指标"><a href="#监控指标" class="headerlink" title="监控指标"></a>监控指标</h3><p>一个好的系统，主要监控以下内容</p><ul><li>集群监控<ul><li>节点资源利用率</li><li>节点数</li><li>运行Pods</li></ul></li><li>Pod监控<ul><li>容器指标</li><li>应用程序【程序占用多少CPU、内存】</li></ul></li></ul><h3 id="监控平台"><a href="#监控平台" class="headerlink" title="监控平台"></a>监控平台</h3><p>使用普罗米修斯【prometheus】 + Grafana 搭建监控平台</p><ul><li><p>prometheus【定时搜索被监控服务的状态】</p><ul><li>开源的</li><li>监控、报警、数据库</li><li>以HTTP协议周期性抓取被监控组件状态</li><li>不需要复杂的集成过程，使用http接口接入即可</li></ul></li><li><p>Grafana</p><ul><li>开源的数据分析和可视化工具</li><li>支持多种数据源</li></ul></li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120082257441.png"></p><h2 id="部署prometheus"><a href="#部署prometheus" class="headerlink" title="部署prometheus"></a>部署prometheus</h2><p>首先需要部署一个守护进程</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120083606298.png"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: node-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: node-exporter</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: node-exporter</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: node-exporter</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: prom/node-exporter</span><br><span class="line">        name: node-exporter</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9100</span><br><span class="line">          protocol: TCP</span><br><span class="line">          name: http</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: node-exporter</span><br><span class="line">  name: node-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 9100</span><br><span class="line">    nodePort: 31672</span><br><span class="line">    protocol: TCP</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: node-exporter</span><br></pre></td></tr></table></figure><p>然后执行下面命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f node-exporter.yaml</span><br></pre></td></tr></table></figure><p>执行完，发现会报错</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120084034160.png"></p><p>这是因为版本不一致的问题，因为发布的正式版本，而这个属于测试版本</p><p>所以我们找到第一行，然后把内容修改为如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改前</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line"><span class="comment"># 修改后 【正式版本发布后，测试版本不能使用】</span></span><br><span class="line">apiVersion: apps/v1</span><br></pre></td></tr></table></figure><p>创建完成后的效果</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120085721454.png"></p><p>然后通过yaml的方式部署prometheus</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120083107594.png"></p><ul><li>configmap：定义一个configmap：存储一些配置文件【不加密】</li><li>prometheus.deploy.yaml：部署一个deployment【包括端口号，资源限制】</li><li>prometheus.svc.yaml：对外暴露的端口</li><li>rbac-setup.yaml：分配一些角色的权限</li></ul><p>下面我们进入目录下，首先部署 rbac-setup.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f rbac-setup.yaml</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120090002150.png"></p><p>然后分别部署</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署configmap</span></span><br><span class="line">kubectl create -f configmap.yaml</span><br><span class="line"><span class="comment"># 部署deployment</span></span><br><span class="line">kubectl create -f prometheus.deploy.yml</span><br><span class="line"><span class="comment"># 部署svc</span></span><br><span class="line">kubectl create -f prometheus.svc.yml</span><br></pre></td></tr></table></figure><p>部署完成后，我们使用下面命令查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120093213576.png"></p><p>在我们部署完成后，即可看到 prometheus 的 pod了，然后通过下面命令，能够看到对应的端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -n kube-system</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121091348752.png"></p><p>通过这个，我们可以看到 <code>prometheus</code> 对外暴露的端口为 30003，访问页面即可对应的图形化界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.177.130:30003</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121091508851.png"></p><p>在上面我们部署完prometheus后，我们还需要来部署grafana</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f grafana-deploy.yaml</span><br></pre></td></tr></table></figure><p>然后执行完后，发现下面的问题</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error: unable to recognize <span class="string">&quot;grafana-deploy.yaml&quot;</span>: no matches <span class="keyword">for</span> kind <span class="string">&quot;Deployment&quot;</span> <span class="keyword">in</span> version <span class="string">&quot;extensions/v1beta1&quot;</span></span><br></pre></td></tr></table></figure><p>我们需要修改如下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加selector</span></span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: grafana</span><br><span class="line">      component: core</span><br></pre></td></tr></table></figure><p>修改完成后，我们继续执行上述代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建deployment</span></span><br><span class="line">kubectl create -f grafana-deploy.yaml</span><br><span class="line"><span class="comment"># 创建svc</span></span><br><span class="line">kubectl create -f grafana-svc.yaml</span><br><span class="line"><span class="comment"># 创建 ing</span></span><br><span class="line">kubectl create -f grafana-ing.yaml</span><br></pre></td></tr></table></figure><p>我们能看到，我们的grafana正在</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120110426534.png"></p><h3 id="配置数据源"><a href="#配置数据源" class="headerlink" title="配置数据源"></a>配置数据源</h3><p>下面我们需要开始打开 Grafana，然后配置数据源，导入数据显示模板</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc -n kube-system</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120111949197.png"></p><p>我们可以通过 ip + 30431 访问我们的 grafana 图形化页面</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201120112048887.png"></p><p>然后输入账号和密码：admin admin</p><p>进入后，我们就需要配置 prometheus 的数据源</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121092012018.png"></p><p> 和 对应的IP【这里IP是我们的ClusterIP】</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121092053215.png"></p><h3 id="设置显示数据的模板"><a href="#设置显示数据的模板" class="headerlink" title="设置显示数据的模板"></a>设置显示数据的模板</h3><p>选择Dashboard，导入我们的模板</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121092312118.png"></p><p>然后输入 315 号模板</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121092418180.png"></p><p>然后选择 prometheus数据源 mydb，导入即可</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121092443266.png"></p><p>导入后的效果如下所示</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201121092610154.png"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes持久化存储</title>
      <link href="/blog/2021/02/01/kubenetes/15/"/>
      <url>/blog/2021/02/01/kubenetes/15/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes持久化存储"><a href="#Kubernetes持久化存储" class="headerlink" title="Kubernetes持久化存储"></a>Kubernetes持久化存储</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前我们有提到数据卷：<code>emptydir</code> ，是本地存储，pod重启，数据就不存在了，需要对数据持久化存储</p><p>对于数据持久化存储【pod重启，数据还存在】，有两种方式</p><ul><li>nfs：网络存储【通过一台服务器来存储】</li></ul><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="持久化服务器上操作"><a href="#持久化服务器上操作" class="headerlink" title="持久化服务器上操作"></a>持久化服务器上操作</h3><ul><li>找一台新的服务器nfs服务端，安装nfs</li><li>设置挂载路径</li></ul><p>使用命令安装nfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>首先创建存放数据的目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /data/nfx</span><br></pre></td></tr></table></figure><p>设置挂载路径</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开文件</span></span><br><span class="line">vim /etc/exports</span><br><span class="line"><span class="comment"># 添加如下内容</span></span><br><span class="line">/data/nfs *(rw,no_root_squash)</span><br></pre></td></tr></table></figure><p>执行完成后，即部署完我们的持久化服务器</p><h3 id="Node节点上操作"><a href="#Node节点上操作" class="headerlink" title="Node节点上操作"></a>Node节点上操作</h3><p>然后需要在k8s集群node节点上安装nfs，这里需要在 node1 和 node2节点上安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><p>执行完成后，会自动帮我们挂载上</p><h3 id="启动nfs服务端"><a href="#启动nfs服务端" class="headerlink" title="启动nfs服务端"></a>启动nfs服务端</h3><p>下面我们回到nfs服务端，启动我们的nfs服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start nfs</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119082047766.png"></p><h3 id="K8s集群部署应用"><a href="#K8s集群部署应用" class="headerlink" title="K8s集群部署应用"></a>K8s集群部署应用</h3><p>最后我们在k8s集群上部署应用，使用nfs持久化存储</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个pv文件</span></span><br><span class="line"><span class="built_in">mkdir</span> pv</span><br><span class="line"><span class="comment"># 进入</span></span><br><span class="line"><span class="built_in">cd</span> pv</span><br></pre></td></tr></table></figure><p>然后创建一个yaml文件  <code>nfs-nginx.yaml</code></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119082317625.png"></p><p>通过这个方式，就挂载到了刚刚我们的nfs数据节点下的 &#x2F;data&#x2F;nfs 目录</p><p>最后就变成了：  &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html    -&gt;  192.168.44.134&#x2F;data&#x2F;nfs   内容是对应的</p><p>我们通过这个 yaml文件，创建一个pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nfs-nginx.yaml</span><br></pre></td></tr></table></figure><p>创建完成后，我们也可以查看日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod nginx-dep1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119083444454.png"></p><p>可以看到，我们的pod已经成功创建出来了，同时下图也是出于Running状态</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119083514247.png"></p><p>下面我们就可以进行测试了，比如现在nfs服务节点上添加数据，然后在看数据是否存在 pod中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入pod中查看</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it nginx-dep1 bash</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119095847548.png"></p><h2 id="PV和PVC"><a href="#PV和PVC" class="headerlink" title="PV和PVC"></a>PV和PVC</h2><p>对于上述的方式，我们都知道，我们的ip 和端口是直接放在我们的容器上的，这样管理起来可能不方便</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119082317625.png"></p><p>所以这里就需要用到 pv  和 pvc的概念了，方便我们配置和管理我们的 ip 地址等元信息</p><p>PV：持久化存储，对存储的资源进行抽象，对外提供可以调用的地方【生产者】</p><p>PVC：用于调用，不需要关心内部实现细节【消费者】</p><p>PV 和 PVC 使得 K8S 集群具备了存储的逻辑抽象能力。使得在配置Pod的逻辑里可以忽略对实际后台存储<br>技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。存储的PV和PVC的这种关系，跟<br>计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变<br>化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集<br>群的使用者即服务的管理员来配置。</p><h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><ul><li>PVC绑定PV</li><li>定义PVC</li><li>定义PV【数据卷定义，指定数据存储服务器的ip、路径、容量和匹配模式】</li></ul><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>创建一个 pvc.yaml</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119101753419.png"></p><p>第一部分是定义一个 deployment，做一个部署</p><ul><li>副本数：3</li><li>挂载路径</li><li>调用：是通过pvc的模式</li></ul><p>然后定义pvc</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119101843498.png"></p><p>然后在创建一个 <code>pv.yaml</code></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119101957777.png"></p><p>然后就可以创建pod了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f pv.yaml</span><br></pre></td></tr></table></figure><p>然后我们就可以通过下面命令，查看我们的 pv  和 pvc之间的绑定关系</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pv, pvc</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119102332786.png"></p><p>到这里为止，我们就完成了我们 pv 和 pvc的绑定操作，通过之前的方式，进入pod中查看内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubect <span class="built_in">exec</span> -it nginx-dep1 bash</span><br></pre></td></tr></table></figure><p>然后查看  &#x2F;usr&#x2F;share&#x2F;nginx.html</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119102448226.png"></p><p>也同样能看到刚刚的内容，其实这种操作和之前我们的nfs是一样的，只是多了一层pvc绑定pv的操作</p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes核心技术Helm</title>
      <link href="/blog/2021/02/01/kubenetes/14/"/>
      <url>/blog/2021/02/01/kubenetes/14/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes核心技术Helm"><a href="#Kubernetes核心技术Helm" class="headerlink" title="Kubernetes核心技术Helm"></a>Kubernetes核心技术Helm</h1><p>Helm就是一个包管理工具【类似于npm】</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/892532-20180224212352306-705544441.png"></p><h2 id="为什么引入Helm"><a href="#为什么引入Helm" class="headerlink" title="为什么引入Helm"></a>为什么引入Helm</h2><p>首先在原来项目中都是基于yaml文件来进行部署发布的，而目前项目大部分微服务化或者模块化，会分成很多个组件来部署，每个组件可能对应一个deployment.yaml,一个service.yaml,一个Ingress.yaml还可能存在各种依赖关系，这样一个项目如果有5个组件，很可能就有15个不同的yaml文件，这些yaml分散存放，如果某天进行项目恢复的话，很难知道部署顺序，依赖关系等，而所有这些包括</p><ul><li>基于yaml配置的集中存放</li><li>基于项目的打包</li><li>组件间的依赖</li></ul><p>但是这种方式部署，会有什么问题呢？</p><ul><li>如果使用之前部署单一应用，少数服务的应用，比较合适</li><li>但如果部署微服务项目，可能有几十个服务，每个服务都有一套yaml文件，需要维护大量的yaml文件，版本管理特别不方便</li></ul><p>Helm的引入，就是为了解决这个问题</p><ul><li>使用Helm可以把这些YAML文件作为整体管理</li><li>实现YAML文件高效复用</li><li>使用helm应用级别的版本管理</li></ul><h2 id="Helm介绍"><a href="#Helm介绍" class="headerlink" title="Helm介绍"></a>Helm介绍</h2><p>Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum&#x2F;apt等，可以很方便的将之前打包好的yaml文件部署到kubernetes上。</p><p>Helm有三个重要概念</p><ul><li>helm：一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理</li><li>Chart：应用描述，一系列用于描述k8s资源相关文件的集合</li><li>Release：基于Chart的部署实体，一个chart被Helm运行后将会生成对应的release，将在K8S中创建出真实的运行资源对象。也就是应用级别的版本管理</li><li>Repository：用于发布和存储Chart的仓库</li></ul><h2 id="Helm组件及架构"><a href="#Helm组件及架构" class="headerlink" title="Helm组件及架构"></a>Helm组件及架构</h2><p>Helm采用客户端&#x2F;服务端架构，有如下组件组成</p><ul><li>Helm CLI是Helm客户端，可以在本地执行</li><li>Tiller是服务器端组件，在Kubernetes集群上运行，并管理Kubernetes应用程序</li><li>Repository是Chart仓库，Helm客户端通过HTTP协议来访问仓库中Chart索引文件和压缩包</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201119095458328.png"></p><h2 id="Helm-v3变化"><a href="#Helm-v3变化" class="headerlink" title="Helm v3变化"></a>Helm v3变化</h2><p>2019年11月13日，Helm团队发布了Helm v3的第一个稳定版本</p><p>该版本主要变化如下</p><ul><li><p>架构变化</p><ul><li>最明显的变化是Tiller的删除</li><li>V3版本删除Tiller</li><li>relesase可以在不同命名空间重用</li></ul></li></ul><p>V3之前</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118171523403.png"></p><p> V3版本</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118171956054.png"></p><h2 id="helm配置"><a href="#helm配置" class="headerlink" title="helm配置"></a>helm配置</h2><p>首先我们需要去 <a href="https://helm.sh/docs/intro/quickstart/">官网下载</a></p><ul><li>第一步，<a href="https://github.com/helm/helm/releases">下载helm</a>安装压缩文件，上传到linux系统中</li><li>第二步，解压helm压缩文件，把解压后的helm目录复制到 usr&#x2F;bin 目录中</li><li>使用命令：helm</li></ul><p>我们都知道yum需要配置yum源，那么helm就就要配置helm源</p><h2 id="helm仓库"><a href="#helm仓库" class="headerlink" title="helm仓库"></a>helm仓库</h2><p>添加仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm repo add 仓库名  仓库地址 </span><br></pre></td></tr></table></figure><p>例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置微软源</span></span><br><span class="line">helm repo add stable http://mirror.azure.cn/kubernetes/charts</span><br><span class="line"><span class="comment"># 配置阿里源</span></span><br><span class="line">helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br><span class="line"><span class="comment"># 配置google源</span></span><br><span class="line">helm repo add google https://kubernetes-charts.storage.googleapis.com/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新</span></span><br><span class="line">helm repo update</span><br></pre></td></tr></table></figure><p>然后可以查看我们添加的仓库地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看全部</span></span><br><span class="line">helm repo list</span><br><span class="line"><span class="comment"># 查看某个</span></span><br><span class="line">helm search repo stable</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118195732281.png"></p><p>或者可以删除我们添加的源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm repo remove stable</span><br></pre></td></tr></table></figure><h2 id="helm基本命令"><a href="#helm基本命令" class="headerlink" title="helm基本命令"></a>helm基本命令</h2><ul><li>chart install</li><li>chart upgrade</li><li>chart rollback</li></ul><h2 id="使用helm快速部署应用"><a href="#使用helm快速部署应用" class="headerlink" title="使用helm快速部署应用"></a>使用helm快速部署应用</h2><h3 id="使用命令搜索应用"><a href="#使用命令搜索应用" class="headerlink" title="使用命令搜索应用"></a>使用命令搜索应用</h3><p>首先我们使用命令，搜索我们需要安装的应用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜索 weave仓库</span></span><br><span class="line">helm search repo weave</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118200603643.png"></p><h3 id="根据搜索内容选择安装"><a href="#根据搜索内容选择安装" class="headerlink" title="根据搜索内容选择安装"></a>根据搜索内容选择安装</h3><p>搜索完成后，使用命令进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install ui aliyun/weave-scope</span><br></pre></td></tr></table></figure><p>可以通过下面命令，来下载yaml文件【如果】</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f weave-scope.yaml</span><br></pre></td></tr></table></figure><p>安装完成后，通过下面命令即可查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm list</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118203727585.png"></p><p>同时可以通过下面命令，查看更新具体的信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm status ui</span><br></pre></td></tr></table></figure><p>但是我们通过查看 svc状态，发现没有对象暴露端口</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118205031343.png"></p><p>所以我们需要修改service的yaml文件，添加NodePort</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit svc ui-weave-scope</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118205129431.png"></p><p>这样就可以对外暴露端口了</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118205147631.png"></p><p>然后我们通过 ip + 32185 即可访问</p><h3 id="如果自己创建Chart"><a href="#如果自己创建Chart" class="headerlink" title="如果自己创建Chart"></a>如果自己创建Chart</h3><p>使用命令，自己创建Chart</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm create mychart</span><br></pre></td></tr></table></figure><p>创建完成后，我们就能看到在当前文件夹下，创建了一个 mychart目录</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118210755621.png"></p><h4 id="目录格式"><a href="#目录格式" class="headerlink" title="目录格式"></a>目录格式</h4><ul><li>templates：编写yaml文件存放到这个目录</li><li>values.yaml：存放的是全局的yaml文件</li><li>chart.yaml：当前chart属性配置信息</li></ul><h3 id="在templates文件夹创建两个文件"><a href="#在templates文件夹创建两个文件" class="headerlink" title="在templates文件夹创建两个文件"></a>在templates文件夹创建两个文件</h3><p>我们创建以下两个</p><ul><li>deployment.yaml</li><li>service.yaml</li></ul><p>我们可以通过下面命令创建出yaml文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导出deployment.yaml</span></span><br><span class="line">kubectl create deployment web1 --image=nginx --dry-run -o yaml &gt; deployment.yaml</span><br><span class="line"><span class="comment"># 导出service.yaml 【可能需要创建 deployment，不然会报错】</span></span><br><span class="line">kubectl expose deployment web1 --port=80 --target-port=80 --<span class="built_in">type</span>=NodePort --dry-run -o yaml &gt; service.yaml</span><br></pre></td></tr></table></figure><h3 id="安装mychart"><a href="#安装mychart" class="headerlink" title="安装mychart"></a>安装mychart</h3><p>执行命令创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install web1 mychart</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118213120916.png"></p><h3 id="应用升级"><a href="#应用升级" class="headerlink" title="应用升级"></a>应用升级</h3><p>当我们修改了mychart中的东西后，就可以进行升级操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm upgrade web1 mychart</span><br></pre></td></tr></table></figure><h2 id="chart模板使用"><a href="#chart模板使用" class="headerlink" title="chart模板使用"></a>chart模板使用</h2><p>通过传递参数，动态渲染模板，yaml内容动态从传入参数生成</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118213630083.png"></p><p>刚刚我们创建mychart的时候，看到有values.yaml文件，这个文件就是一些全局的变量，然后在templates中能取到变量的值，下面我们可以利用这个，来完成动态模板</p><ul><li>在values.yaml定义变量和值</li><li>具体yaml文件，获取定义变量值</li><li>yaml文件中大题有几个地方不同<ul><li>image</li><li>tag</li><li>label</li><li>port</li><li>replicas</li></ul></li></ul><h3 id="定义变量和值"><a href="#定义变量和值" class="headerlink" title="定义变量和值"></a>定义变量和值</h3><p>在values.yaml定义变量和值</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118214050899.png"></p><h3 id="获取变量和值"><a href="#获取变量和值" class="headerlink" title="获取变量和值"></a>获取变量和值</h3><p>我们通过表达式形式 使用全局变量  <code>&#123;&#123;.Values.变量名称&#125;&#125; </code></p><p>例如： <code>&#123;&#123;.Release.Name&#125;&#125;</code></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118214413203.png"></p><h3 id="安装应用"><a href="#安装应用" class="headerlink" title="安装应用"></a>安装应用</h3><p>在我们修改完上述的信息后，就可以尝试的创建应用了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm install --dry-run web2 mychart</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118214727058.png"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes核心技术Ingress</title>
      <link href="/blog/2021/02/01/kubenetes/13/"/>
      <url>/blog/2021/02/01/kubenetes/13/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes核心技术Ingress"><a href="#Kubernetes核心技术Ingress" class="headerlink" title="Kubernetes核心技术Ingress"></a>Kubernetes核心技术Ingress</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>原来我们需要将端口号对外暴露，通过 ip + 端口号就可以进行访问</p><p>原来是使用Service中的NodePort来实现</p><ul><li>在每个节点上都会启动端口</li><li>在访问的时候通过任何节点，通过ip + 端口号就能实现访问</li></ul><p>但是NodePort还存在一些缺陷</p><ul><li>因为端口不能重复，所以每个端口只能使用一次，一个端口对应一个应用</li><li>实际访问中都是用域名，根据不同域名跳转到不同端口服务中</li></ul><h2 id="Ingress和Pod关系"><a href="#Ingress和Pod关系" class="headerlink" title="Ingress和Pod关系"></a>Ingress和Pod关系</h2><p>pod 和 ingress 是通过service进行关联的，而ingress作为统一入口，由service关联一组pod中</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118102637839.png"></p><ul><li>首先service就是关联我们的pod</li><li>然后ingress作为入口，首先需要到service，然后发现一组pod</li><li>发现pod后，就可以做负载均衡等操作</li></ul><h2 id="Ingress工作流程"><a href="#Ingress工作流程" class="headerlink" title="Ingress工作流程"></a>Ingress工作流程</h2><p>在实际的访问中，我们都是需要维护很多域名， a.com  和  b.com</p><p>然后不同的域名对应的不同的Service，然后service管理不同的pod</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118102858617.png"></p><p>需要注意，ingress不是内置的组件，需要我们单独的安装</p><h2 id="使用Ingress"><a href="#使用Ingress" class="headerlink" title="使用Ingress"></a>使用Ingress</h2><p>步骤如下所示</p><ul><li>部署ingress Controller【需要下载官方的】</li><li>创建ingress规则【对哪个Pod、名称空间配置规则】</li></ul><h3 id="创建Nginx-Pod"><a href="#创建Nginx-Pod" class="headerlink" title="创建Nginx Pod"></a>创建Nginx Pod</h3><p>创建一个nginx应用，然后对外暴露端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建pod</span></span><br><span class="line">kubectl create deployment web --image=nginx</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><p>对外暴露端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --target-port=80 --<span class="built_in">type</span>:NodePort</span><br></pre></td></tr></table></figure><h3 id="部署-ingress-controller"><a href="#部署-ingress-controller" class="headerlink" title="部署 ingress controller"></a>部署 ingress controller</h3><p>下面我们来通过yaml的方式，部署我们的ingress，配置文件如下所示</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118105427248.png"></p><p>这个文件里面，需要注意的是 hostNetwork: true，改成ture是为了让后面访问到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ingress-con.yaml</span><br></pre></td></tr></table></figure><p>通过这种方式，其实我们在外面就能访问，这里还需要在外面添加一层</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f ingress-con.yaml</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118111256631.png"></p><p>最后通过下面命令，查看是否成功部署 ingress</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n ingress-nginx</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118111424735.png"></p><h3 id="创建ingress规则文件"><a href="#创建ingress规则文件" class="headerlink" title="创建ingress规则文件"></a>创建ingress规则文件</h3><p>创建ingress规则文件，ingress-h.yaml</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118111700534.png"></p><h3 id="添加域名访问规则"><a href="#添加域名访问规则" class="headerlink" title="添加域名访问规则"></a>添加域名访问规则</h3><p>在windows 的 hosts文件，添加域名访问规则【因为我们没有域名解析，所以只能这样做】</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118112029820.png"></p><p>最后通过域名就能访问</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118112212519.png"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes集群安全机制</title>
      <link href="/blog/2021/02/01/kubenetes/12/"/>
      <url>/blog/2021/02/01/kubenetes/12/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes集群安全机制"><a href="#Kubernetes集群安全机制" class="headerlink" title="Kubernetes集群安全机制"></a>Kubernetes集群安全机制</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>当我们访问K8S集群时，需要经过三个步骤完成具体操作</p><ul><li>认证</li><li>鉴权【授权】</li><li>准入控制</li></ul><p>进行访问的时候，都需要经过 apiserver， apiserver做统一协调，比如门卫</p><ul><li>访问过程中，需要证书、token、或者用户名和密码</li><li>如果访问pod需要serviceAccount</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118092356107.png"></p><h3 id="认证"><a href="#认证" class="headerlink" title="认证"></a>认证</h3><p>对外不暴露8080端口，只能内部访问，对外使用的端口6443</p><p>客户端身份认证常用方式</p><ul><li>https证书认证，基于ca证书</li><li>http token认证，通过token来识别用户</li><li>http基本认证，用户名 + 密码认证</li></ul><h3 id="鉴权"><a href="#鉴权" class="headerlink" title="鉴权"></a>鉴权</h3><p>基于RBAC进行鉴权操作</p><p>基于角色访问控制</p><h3 id="准入控制"><a href="#准入控制" class="headerlink" title="准入控制"></a>准入控制</h3><p>就是准入控制器的列表，如果列表有请求内容就通过，没有的话 就拒绝</p><h2 id="RBAC介绍"><a href="#RBAC介绍" class="headerlink" title="RBAC介绍"></a>RBAC介绍</h2><p>基于角色的访问控制，为某个角色设置访问内容，然后用户分配该角色后，就拥有该角色的访问权限</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118093949893.png"></p><p>k8s中有默认的几个角色</p><ul><li>role：特定命名空间访问权限</li><li>ClusterRole：所有命名空间的访问权限</li></ul><p>角色绑定</p><ul><li>roleBinding：角色绑定到主体</li><li>ClusterRoleBinding：集群角色绑定到主体</li></ul><p>主体</p><ul><li>user：用户</li><li>group：用户组</li><li>serviceAccount：服务账号</li></ul><h2 id="RBAC实现鉴权"><a href="#RBAC实现鉴权" class="headerlink" title="RBAC实现鉴权"></a>RBAC实现鉴权</h2><ul><li>创建命名空间</li></ul><h3 id="创建命名空间"><a href="#创建命名空间" class="headerlink" title="创建命名空间"></a>创建命名空间</h3><p>我们可以首先查看已经存在的命名空间</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get namespace</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118094516426.png"></p><p>然后我们创建一个自己的命名空间  roledemo</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create ns roledemo</span><br></pre></td></tr></table></figure><h3 id="命名空间创建Pod"><a href="#命名空间创建Pod" class="headerlink" title="命名空间创建Pod"></a>命名空间创建Pod</h3><p>为什么要创建命名空间？因为如果不创建命名空间的话，默认是在default下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run nginx --image=nginx -n roledemo</span><br></pre></td></tr></table></figure><h3 id="创建角色"><a href="#创建角色" class="headerlink" title="创建角色"></a>创建角色</h3><p>我们通过 rbac-role.yaml进行创建</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118094851338.png"></p><p>tip：这个角色只对pod 有 get、list权限</p><p>然后通过 yaml创建我们的role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建</span></span><br><span class="line">kubectl apply -f rbac-role.yaml</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">kubectl get role -n roledemo</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118095141786.png"></p><h3 id="创建角色绑定"><a href="#创建角色绑定" class="headerlink" title="创建角色绑定"></a>创建角色绑定</h3><p>我们还是通过 role-rolebinding.yaml 的方式，来创建我们的角色绑定</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118095248052.png"></p><p>然后创建我们的角色绑定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建角色绑定</span></span><br><span class="line">kubectl apply -f rbac-rolebinding.yaml</span><br><span class="line"><span class="comment"># 查看角色绑定</span></span><br><span class="line">kubectl get role, rolebinding -n roledemo</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118095357067.png"></p><h3 id="使用证书识别身份"><a href="#使用证书识别身份" class="headerlink" title="使用证书识别身份"></a>使用证书识别身份</h3><p>我们首先得有一个 rbac-user.sh 证书脚本</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118095541427.png"></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118095627954.png"></p><p>这里包含了很多证书文件，在TSL目录下，需要复制过来</p><p>通过下面命令执行我们的脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./rbac-user.sh</span><br></pre></td></tr></table></figure><p>最后我们进行测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用get命令查看 pod 【有权限】</span></span><br><span class="line">kubectl get pods -n roledemo</span><br><span class="line"><span class="comment"># 用get命令查看svc 【没权限】</span></span><br><span class="line">kubectl get svc -n roledmeo</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118100051043.png"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes配置管理</title>
      <link href="/blog/2021/02/01/kubenetes/11/"/>
      <url>/blog/2021/02/01/kubenetes/11/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes配置管理"><a href="#Kubernetes配置管理" class="headerlink" title="Kubernetes配置管理"></a>Kubernetes配置管理</h1><h2 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h2><p>Secret的主要作用就是加密数据，然后存在etcd里面，让Pod容器以挂载Volume方式进行访问</p><p>场景：用户名 和 密码进行加密</p><p>一般场景的是对某个字符串进行base64编码 进行加密</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -n <span class="string">&#x27;admin&#x27;</span> | <span class="built_in">base64</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117212037668.png"></p><h3 id="变量形式挂载到Pod"><a href="#变量形式挂载到Pod" class="headerlink" title="变量形式挂载到Pod"></a>变量形式挂载到Pod</h3><ul><li>创建secret加密数据的yaml文件    secret.yaml</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117212124476.png"></p><p>然后使用下面命令创建一个pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f secret.yaml</span><br></pre></td></tr></table></figure><p>通过get命令查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118084010980.png"></p><p>然后我们通过下面的命令，进入到我们的容器内部</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it mypod bash</span><br></pre></td></tr></table></figure><p>然后我们就可以输出我们的值，这就是以变量的形式挂载到我们的容器中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出用户</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$SECRET_USERNAME</span></span><br><span class="line"><span class="comment"># 输出密码</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$SECRET_PASSWORD</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118084137942.png"></p><p>最后如果我们要删除这个Pod，就可以使用这个命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f secret-val.yaml</span><br></pre></td></tr></table></figure><h3 id="数据卷形式挂载"><a href="#数据卷形式挂载" class="headerlink" title="数据卷形式挂载"></a>数据卷形式挂载</h3><p>首先我们创建一个 secret-val.yaml 文件</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118084321590.png"></p><p>然后创建我们的 Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据配置创建容器</span></span><br><span class="line">kubectl apply -f secret-val.yaml</span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">kubectl <span class="built_in">exec</span> -it mypod bash</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line"><span class="built_in">ls</span> /etc/foo</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118084707478.png"></p><h2 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h2><p>ConfigMap作用是存储不加密的数据到etcd中，让Pod以变量或数据卷Volume挂载到容器中</p><p>应用场景：配置文件</p><h3 id="创建配置文件"><a href="#创建配置文件" class="headerlink" title="创建配置文件"></a>创建配置文件</h3><p>首先我们需要创建一个配置文件 <code>redis.properties</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">redis.port=127.0.0.1</span><br><span class="line">redis.port=6379</span><br><span class="line">redis.password=123456</span><br></pre></td></tr></table></figure><h3 id="创建ConfigMap"><a href="#创建ConfigMap" class="headerlink" title="创建ConfigMap"></a>创建ConfigMap</h3><p>我们使用命令创建configmap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap redis-config --from-file=redis.properties</span><br></pre></td></tr></table></figure><p>然后查看详细信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe cm redis-config</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118085503534.png"></p><h3 id="Volume数据卷形式挂载"><a href="#Volume数据卷形式挂载" class="headerlink" title="Volume数据卷形式挂载"></a>Volume数据卷形式挂载</h3><p>首先我们需要创建一个 <code>cm.yaml</code></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118085847424.png"></p><p>然后使用该yaml创建我们的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建</span></span><br><span class="line">kubectl apply -f cm.yaml</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118090634869.png"></p><p>最后我们通过命令就可以查看结果输出了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs mypod</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118090712780.png"></p><h3 id="以变量的形式挂载Pod"><a href="#以变量的形式挂载Pod" class="headerlink" title="以变量的形式挂载Pod"></a>以变量的形式挂载Pod</h3><p>首先我们也有一个 myconfig.yaml文件，声明变量信息，然后以configmap创建</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118090911260.png"></p><p>然后我们就可以创建我们的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建pod</span></span><br><span class="line">kubectl apply -f myconfig.yaml</span><br><span class="line"><span class="comment"># 获取</span></span><br><span class="line">kubectl get cm</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118091042287.png"></p><p>然后我们创建完该pod后，我们就需要在创建一个  config-var.yaml 来使用我们的配置信息</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118091249520.png"></p><p>最后我们查看输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs mypod</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201118091448252.png"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes控制器Controller详解</title>
      <link href="/blog/2021/02/01/kubenetes/10/"/>
      <url>/blog/2021/02/01/kubenetes/10/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes控制器Controller详解"><a href="#Kubernetes控制器Controller详解" class="headerlink" title="Kubernetes控制器Controller详解"></a>Kubernetes控制器Controller详解</h1><h2 id="Statefulset"><a href="#Statefulset" class="headerlink" title="Statefulset"></a>Statefulset</h2><p>Statefulset主要是用来部署有状态应用</p><p>对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。</p><h3 id="无状态应用"><a href="#无状态应用" class="headerlink" title="无状态应用"></a>无状态应用</h3><p>我们原来使用 deployment，部署的都是无状态的应用，那什么是无状态应用？</p><ul><li>认为Pod都是一样的</li><li>没有顺序要求</li><li>不考虑应用在哪个node上运行</li><li>能够进行随意伸缩和扩展</li></ul><h3 id="有状态应用"><a href="#有状态应用" class="headerlink" title="有状态应用"></a>有状态应用</h3><p>上述的因素都需要考虑到</p><ul><li>让每个Pod独立的</li><li>让每个Pod独立的，保持Pod启动顺序和唯一性</li><li>唯一的网络标识符，持久存储</li><li>有序，比如mysql中的主从</li></ul><p>适合StatefulSet的业务包括数据库服务MySQL 和 PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务</p><p>StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。</p><p>使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供<br>高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。</p><h3 id="部署有状态应用"><a href="#部署有状态应用" class="headerlink" title="部署有状态应用"></a>部署有状态应用</h3><p>无头service， ClusterIp：none</p><p>这里就需要使用 StatefulSet部署有状态应用</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117202950336.png"></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117203130867.png"></p><p>然后通过查看pod，能否发现每个pod都有唯一的名称</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117203217016.png"></p><p>然后我们在查看service，发现是无头的service</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117203245641.png"></p><p>这里有状态的约定，肯定不是简简单单通过名称来进行约定，而是更加复杂的操作</p><ul><li>deployment：是有身份的，有唯一标识</li><li>statefulset：根据主机名 + 按照一定规则生成域名</li></ul><p>每个pod有唯一的主机名，并且有唯一的域名</p><ul><li>格式：主机名称.service名称.名称空间.svc.cluster.local</li><li>举例：nginx-statefulset-0.default.svc.cluster.local</li></ul><h2 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h2><p>DaemonSet 即后台支撑型服务，主要是用来部署守护进程</p><p>长期伺服型和批处理型的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类的Pod运行；而后台支撑型服务的核心关注点在K8S集群中的节点(物理机或虚拟机)，要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点，也可能是通过 nodeSelector选定的一些特定节点。典型的后台支撑型服务包括：存储、日志和监控等。在每个节点上支撑K8S集群运行的服务。</p><p>守护进程在我们每个节点上，运行的是同一个pod，新加入的节点也同样运行在同一个pod里面</p><ul><li>例子：在每个node节点安装数据采集工具</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117204430836.png"></p><p>这里是不是一个FileBeat镜像，主要是为了做日志采集工作</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117204810350.png"></p><p>进入某个 Pod里面，进入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it ds-test-cbk6v bash</span><br></pre></td></tr></table></figure><p>通过该命令后，我们就能看到我们内部收集的日志信息了</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117204912838.png"></p><h2 id="Job和CronJob"><a href="#Job和CronJob" class="headerlink" title="Job和CronJob"></a>Job和CronJob</h2><p>一次性任务 和 定时任务</p><ul><li>一次性任务：一次性执行完就结束</li><li>定时任务：周期性执行</li></ul><p>Job是K8S中用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别就是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的 spec.completions 策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功行任务保证有N个任务全部成功；工作队列性任务根据应用确定的全局成功而标志成功。</p><h3 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h3><p>Job也即一次性任务</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117205635945.png"></p><p>使用下面命令，能够看到目前已经存在的Job</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get <span class="built_in">jobs</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117205948374.png"></p><p>在计算完成后，通过命令查看，能够发现该任务已经完成</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117210031725.png"></p><p>我们可以通过查看日志，查看到一次性任务的结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs pi-qpqff</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117210110343.png"></p><h3 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h3><p>定时任务，cronjob.yaml如下所示</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117210309069.png"></p><p>这里面的命令就是每个一段时间，这里是通过 cron 表达式配置的，通过 schedule字段</p><p>然后下面命令就是每个一段时间输出 </p><p>我们首先用上述的配置文件，创建一个定时任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f cronjob.yaml</span><br></pre></td></tr></table></figure><p>创建完成后，我们就可以通过下面命令查看定时任务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get cronjobs</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117210611783.png"></p><p>我们可以通过日志进行查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs hello-1599100140-wkn79</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117210722556.png"></p><p>然后每次执行，就会多出一个 pod</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117210751068.png"></p><h2 id="删除svc-和-statefulset"><a href="#删除svc-和-statefulset" class="headerlink" title="删除svc 和 statefulset"></a>删除svc 和 statefulset</h2><p>使用下面命令，可以删除我们添加的svc 和 statefulset</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete svc web</span><br><span class="line"></span><br><span class="line">kubectl delete statefulset --all</span><br></pre></td></tr></table></figure><h2 id="Replication-Controller"><a href="#Replication-Controller" class="headerlink" title="Replication Controller"></a>Replication Controller</h2><p>Replication Controller 简称 <strong>RC</strong>，是K8S中的复制控制器。RC是K8S集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。</p><p>即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有一个Pod在运行。RC是K8S中较早期的技术概念，只适用于长期伺服型的业务类型，比如控制Pod提供高可用的Web服务。</p><h3 id="Replica-Set"><a href="#Replica-Set" class="headerlink" title="Replica Set"></a>Replica Set</h3><p>Replica Set 检查 RS，也就是副本集。RS是新一代的RC，提供同样高可用能力，区别主要在于RS后来居上，能够支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数来使用</p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes核心技术Service</title>
      <link href="/blog/2021/02/01/kubenetes/9/"/>
      <url>/blog/2021/02/01/kubenetes/9/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes核心技术Service"><a href="#Kubernetes核心技术Service" class="headerlink" title="Kubernetes核心技术Service"></a>Kubernetes核心技术Service</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面我们了解到 Deployment 只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。</p><p>要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的后端服务实例。在K8S集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。</p><p>在K8S集群中，微服务的负载均衡是由kube-proxy实现的。kube-proxy是k8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8S的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端使用反向代理作负载均衡，还要进一步解决反向代理的高可用问题。</p><h2 id="Service存在的意义"><a href="#Service存在的意义" class="headerlink" title="Service存在的意义"></a>Service存在的意义</h2><h3 id="防止Pod失联【服务发现】"><a href="#防止Pod失联【服务发现】" class="headerlink" title="防止Pod失联【服务发现】"></a>防止Pod失联【服务发现】</h3><p>因为Pod每次创建都对应一个IP地址，而这个IP地址是短暂的，每次随着Pod的更新都会变化，假设当我们的前端页面有多个Pod时候，同时后端也多个Pod，这个时候，他们之间的相互访问，就需要通过注册中心，拿到Pod的IP地址，然后去访问对应的Pod</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117093606710.png"></p><h3 id="定义Pod访问策略【负载均衡】"><a href="#定义Pod访问策略【负载均衡】" class="headerlink" title="定义Pod访问策略【负载均衡】"></a>定义Pod访问策略【负载均衡】</h3><p>页面前端的Pod访问到后端的Pod，中间会通过Service一层，而Service在这里还能做负载均衡，负载均衡的策略有很多种实现策略，例如：</p><ul><li>随机</li><li>轮询</li><li>响应比</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117093902459.png"></p><h2 id="Pod和Service的关系"><a href="#Pod和Service的关系" class="headerlink" title="Pod和Service的关系"></a>Pod和Service的关系</h2><p>这里Pod 和 Service 之间还是根据 label 和 selector 建立关联的 【和Controller一样】</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117094142491.png"></p><p>我们在访问service的时候，其实也是需要有一个ip地址，这个ip肯定不是pod的ip地址，而是 虚拟IP <code>vip</code> </p><h2 id="Service常用类型"><a href="#Service常用类型" class="headerlink" title="Service常用类型"></a>Service常用类型</h2><p>Service常用类型有三种</p><ul><li>ClusterIp：集群内部访问</li><li>NodePort：对外访问应用使用</li><li>LoadBalancer：对外访问应用使用，公有云</li></ul><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>我们可以导出一个文件 包含service的配置信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --target-port=80 --dry-run -o yaml &gt; service.yaml</span><br></pre></td></tr></table></figure><p>service.yaml 如下所示</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>如果我们没有做设置的话，默认使用的是第一种方式 ClusterIp，也就是只能在集群内部使用，我们可以添加一个type字段，用来设置我们的service类型</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">loadBalancer:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>修改完命令后，我们使用创建一个pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f service.yaml</span><br></pre></td></tr></table></figure><p>然后能够看到，已经成功修改为 NodePort类型了，最后剩下的一种方式就是LoadBalanced：对外访问应用使用公有云</p><p>node一般是在内网进行部署，而外网一般是不能访问到的，那么如何访问的呢？</p><ul><li>找到一台可以通过外网访问机器，安装nginx，反向代理</li><li>手动把可以访问的节点添加到nginx中</li></ul><p>如果我们使用LoadBalancer，就会有负载均衡的控制器，类似于nginx的功能，就不需要自己添加到nginx上</p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes核心技术-Controller</title>
      <link href="/blog/2021/02/01/kubenetes/8/"/>
      <url>/blog/2021/02/01/kubenetes/8/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes核心技术-Controller"><a href="#Kubernetes核心技术-Controller" class="headerlink" title="Kubernetes核心技术-Controller"></a>Kubernetes核心技术-Controller</h1><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li>什么是Controller</li><li>Pod和Controller的关系</li><li>Deployment控制器应用场景</li><li>yaml文件字段说明</li><li>Deployment控制器部署应用</li><li>升级回滚</li><li>弹性伸缩</li></ul><h2 id="什么是Controller"><a href="#什么是Controller" class="headerlink" title="什么是Controller"></a>什么是Controller</h2><p>Controller是在集群上管理和运行容器的对象，Controller是实际存在的，Pod是虚拟机的</p><h2 id="Pod和Controller的关系"><a href="#Pod和Controller的关系" class="headerlink" title="Pod和Controller的关系"></a>Pod和Controller的关系</h2><p>Pod是通过Controller实现应用的运维，比如弹性伸缩，滚动升级等</p><p>Pod 和 Controller之间是通过label标签来建立关系，同时Controller又被称为控制器工作负载</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116092431237.png"></p><h2 id="Deployment控制器应用"><a href="#Deployment控制器应用" class="headerlink" title="Deployment控制器应用"></a>Deployment控制器应用</h2><ul><li>Deployment控制器可以部署无状态应用</li><li>管理Pod和ReplicaSet</li><li>部署，滚动升级等功能</li><li>应用场景：web服务，微服务</li></ul><p>Deployment表示用户对K8S集群的一次更新操作。Deployment是一个比RS( Replica Set, RS) 应用模型更广的 API 对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新 RS 中副本数增加到理想状态，将旧RS中的副本数减少到0的复合操作。</p><p>这样一个复合操作用一个RS是不好描述的，所以用一个更通用的Deployment来描述。以K8S的发展方向，未来对所有长期伺服型的业务的管理，都会通过Deployment来管理。</p><h2 id="Deployment部署应用"><a href="#Deployment部署应用" class="headerlink" title="Deployment部署应用"></a>Deployment部署应用</h2><p>之前我们也使用Deployment部署过应用，如下代码所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectrl create deployment web --image=nginx</span><br></pre></td></tr></table></figure><p>但是上述代码不是很好的进行复用，因为每次我们都需要重新输入代码，所以我们都是通过YAML进行配置</p><p>但是我们可以尝试使用上面的代码创建一个镜像【只是尝试，不会创建】</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment web --image=nginx --dry-run -o yaml &gt; nginx.yaml</span><br></pre></td></tr></table></figure><p>然后输出一个yaml配置文件 <code>nginx.yml</code> ，配置文件如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx</span><br><span class="line">        resources: &#123;&#125;</span><br><span class="line">status: &#123;&#125;</span><br></pre></td></tr></table></figure><p>我们看到的 selector 和 label 就是我们Pod 和 Controller之间建立关系的桥梁</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116093638951.png"></p><h3 id="使用YAML创建Pod"><a href="#使用YAML创建Pod" class="headerlink" title="使用YAML创建Pod"></a>使用YAML创建Pod</h3><p>通过刚刚的代码，我们已经生成了YAML文件，下面我们就可以使用该配置文件快速创建Pod镜像了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx.yaml</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116094046007.png"></p><p>但是因为这个方式创建的，我们只能在集群内部进行访问，所以我们还需要对外暴露端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --<span class="built_in">type</span>=NodePort --target-port=80 --name=web1</span><br></pre></td></tr></table></figure><p>关于上述命令，有几次参数</p><ul><li>–port：就是我们内部的端口号</li><li>–target-get-port：就是暴露外面访问的端口号</li><li>–name：名称</li><li>–type：类型</li></ul><p>同理，我们一样可以导出对应的配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment web --port=80 --<span class="built_in">type</span>=NodePort --target-port=80 --name=web1 -o yaml &gt; web1.yaml</span><br></pre></td></tr></table></figure><p>得到的web1.yaml如下所示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: <span class="string">&quot;2020-11-16T02:26:53Z&quot;</span></span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">  managedFields:</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:metadata:</span><br><span class="line">        f:labels:</span><br><span class="line">          .: &#123;&#125;</span><br><span class="line">          f:app: &#123;&#125;</span><br><span class="line">      f:spec:</span><br><span class="line">        f:externalTrafficPolicy: &#123;&#125;</span><br><span class="line">        f:ports:</span><br><span class="line">          .: &#123;&#125;</span><br><span class="line">          k:&#123;<span class="string">&quot;port&quot;</span>:80,<span class="string">&quot;protocol&quot;</span>:<span class="string">&quot;TCP&quot;</span>&#125;:</span><br><span class="line">            .: &#123;&#125;</span><br><span class="line">            f:port: &#123;&#125;</span><br><span class="line">            f:protocol: &#123;&#125;</span><br><span class="line">            f:targetPort: &#123;&#125;</span><br><span class="line">        f:selector:</span><br><span class="line">          .: &#123;&#125;</span><br><span class="line">          f:app: &#123;&#125;</span><br><span class="line">        f:sessionAffinity: &#123;&#125;</span><br><span class="line">        f:<span class="built_in">type</span>: &#123;&#125;</span><br><span class="line">    manager: kubectl</span><br><span class="line">    operation: Update</span><br><span class="line">    time: <span class="string">&quot;2020-11-16T02:26:53Z&quot;</span></span><br><span class="line">  name: web2</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">&quot;113693&quot;</span></span><br><span class="line">  selfLink: /api/v1/namespaces/default/services/web2</span><br><span class="line">  uid: d570437d-a6b4-4456-8dfb-950f09534516</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: 10.104.174.145</span><br><span class="line">  externalTrafficPolicy: Cluster</span><br><span class="line">  ports:</span><br><span class="line">  - nodePort: 32639</span><br><span class="line">    port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: web</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">status:</span><br><span class="line">  loadBalancer: &#123;&#125;</span><br></pre></td></tr></table></figure><p>然后我们可以通过下面的命令来查看对外暴露的服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods,svc</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116104021357.png"></p><p>然后我们访问对应的url，即可看到 nginx了 <code>http://192.168.177.130:32639/</code></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116104131968.png"></p><h2 id="升级回滚和弹性伸缩"><a href="#升级回滚和弹性伸缩" class="headerlink" title="升级回滚和弹性伸缩"></a>升级回滚和弹性伸缩</h2><ul><li>升级：  假设从版本为1.14 升级到 1.15 ，这就叫应用的升级【升级可以保证服务不中断】</li><li>回滚：从版本1.15 变成 1.14，这就叫应用的回滚</li><li>弹性伸缩：我们根据不同的业务场景，来改变Pod的数量对外提供服务，这就是弹性伸缩</li></ul><h3 id="应用升级和回滚"><a href="#应用升级和回滚" class="headerlink" title="应用升级和回滚"></a>应用升级和回滚</h3><p>首先我们先创建一个 1.14版本的Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.14</span><br><span class="line">        name: nginx</span><br><span class="line">        resources: &#123;&#125;</span><br><span class="line">status: &#123;&#125;</span><br></pre></td></tr></table></figure><p>我们先指定版本为1.14，然后开始创建我们的Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f nginx.yaml</span><br></pre></td></tr></table></figure><p>同时，我们使用docker images命令，就能看到我们成功拉取到了一个 1.14版本的镜像</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116105710966.png"></p><p>我们使用下面的命令，可以将nginx从 1.14 升级到 1.15</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">set</span> image deployment web nginx=nginx:1.15</span><br></pre></td></tr></table></figure><p>在我们执行完命令后，能看到升级的过程</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116105847069.png"></p><ul><li>首先是开始的nginx 1.14版本的Pod在运行，然后 1.15版本的在创建</li><li>然后在1.15版本创建完成后，就会暂停1.14版本</li><li>最后把1.14版本的Pod移除，完成我们的升级</li></ul><p>我们在下载 1.15版本，容器就处于ContainerCreating状态，然后下载完成后，就用 1.15版本去替换1.14版本了，这么做的好处就是：升级可以保证服务不中断</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116111614085.png"></p><p>我们到我们的node2节点上，查看我们的 docker images;</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116111315000.png"></p><p>能够看到，我们已经成功拉取到了 1.15版本的nginx了</p><h4 id="查看升级状态"><a href="#查看升级状态" class="headerlink" title="查看升级状态"></a>查看升级状态</h4><p>下面可以，查看升级状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout status deployment web</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116112139645.png"></p><h4 id="查看历史版本"><a href="#查看历史版本" class="headerlink" title="查看历史版本"></a>查看历史版本</h4><p>我们还可以查看历史版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout <span class="built_in">history</span> deployment web</span><br></pre></td></tr></table></figure><h4 id="应用回滚"><a href="#应用回滚" class="headerlink" title="应用回滚"></a>应用回滚</h4><p>我们可以使用下面命令，完成回滚操作，也就是回滚到上一个版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment web</span><br></pre></td></tr></table></figure><p>然后我们就可以查看状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout status deployment web</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201116112524601.png"></p><p>同时我们还可以回滚到指定版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployment web --to-revision=2</span><br></pre></td></tr></table></figure><h3 id="弹性伸缩"><a href="#弹性伸缩" class="headerlink" title="弹性伸缩"></a>弹性伸缩</h3><p>弹性伸缩，也就是我们通过命令一下创建多个副本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment web --replicas=10</span><br></pre></td></tr></table></figure><p>能够清晰看到，我们一下创建了10个副本</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201117092841865.png"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes核心技术Pod</title>
      <link href="/blog/2021/02/01/kubenetes/7/"/>
      <url>/blog/2021/02/01/kubenetes/7/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes核心技术Pod"><a href="#Kubernetes核心技术Pod" class="headerlink" title="Kubernetes核心技术Pod"></a>Kubernetes核心技术Pod</h1><h2 id="Pod概述"><a href="#Pod概述" class="headerlink" title="Pod概述"></a>Pod概述</h2><p>Pod是K8S系统中可以创建和管理的最小单元，是资源对象模型中由用户创建或部署的最小资源对象模型，也是在K8S上运行容器化应用的资源对象，其它的资源对象都是用来支撑或者扩展Pod对象功能的，比如控制器对象是用来管控Pod对象的，Service或者Ingress资源对象是用来暴露Pod引用对象的，PersistentVolume资源对象是用来为Pod提供存储等等，K8S不会直接处理容器，而是Pod，Pod是由一个或多个container组成。</p><p>Pod是Kubernetes的最重要概念，每一个Pod都有一个特殊的被称为 “根容器”的Pause容器。Pause容器对应的镜像属于Kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114185528215.png"></p><h3 id="Pod基本概念"><a href="#Pod基本概念" class="headerlink" title="Pod基本概念"></a>Pod基本概念</h3><ul><li>最小部署的单元</li><li>Pod里面是由一个或多个容器组成【一组容器的集合】</li><li>一个pod中的容器是共享网络命名空间</li><li>Pod是短暂的</li><li>每个Pod包含一个或多个紧密相关的用户业务容器</li></ul><h3 id="Pod存在的意义"><a href="#Pod存在的意义" class="headerlink" title="Pod存在的意义"></a>Pod存在的意义</h3><ul><li>创建容器使用docker，一个docker对应一个容器，一个容器运行一个应用进程</li><li>Pod是多进程设计，运用多个应用程序，也就是一个Pod里面有多个容器，而一个容器里面运行一个应用程序</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114190018948.png"></p><ul><li>Pod的存在是为了亲密性应用<ul><li>两个应用之间进行交互</li><li>网络之间的调用【通过127.0.0.1 或 socket】</li><li>两个应用之间需要频繁调用</li></ul></li></ul><p>Pod是在K8S集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。同时Pod对多容器的支持是K8S中最基础的设计理念。在生产环境中，通常是由不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。</p><p>Pod是K8S集群中所有业务类型的基础，可以把Pod看作运行在K8S集群上的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8S的业务主要可以分为以下几种</p><ul><li>长期伺服型：long-running</li><li>批处理型：batch</li><li>节点后台支撑型：node-daemon</li><li>有状态应用型：stateful application</li></ul><p>上述的几种类型，分别对应的小机器人控制器为：Deployment、Job、DaemonSet 和 StatefulSet  (后面将介绍控制器)</p><h2 id="Pod实现机制"><a href="#Pod实现机制" class="headerlink" title="Pod实现机制"></a>Pod实现机制</h2><p>主要有以下两大机制</p><ul><li>共享网络</li><li>共享存储</li></ul><h3 id="共享网络"><a href="#共享网络" class="headerlink" title="共享网络"></a>共享网络</h3><p>容器本身之间相互隔离的，一般是通过 <strong>namespace</strong> 和 <strong>group</strong> 进行隔离，那么Pod里面的容器如何实现通信？</p><ul><li>首先需要满足前提条件，也就是容器都在同一个<strong>namespace</strong>之间</li></ul><p>关于Pod实现原理，首先会在Pod会创建一个根容器： <code>pause容器</code>，然后我们在创建业务容器 【nginx，redis 等】，在我们创建业务容器的时候，会把它添加到 <code>info容器</code> 中</p><p>而在 <code>info容器</code> 中会独立出  ip地址，mac地址，port 等信息，然后实现网络的共享</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114190913859.png"></p><p>完整步骤如下</p><ul><li>通过 Pause 容器，把其它业务容器加入到Pause容器里，让所有业务容器在同一个名称空间中，可以实现网络共享</li></ul><h3 id="共享存储"><a href="#共享存储" class="headerlink" title="共享存储"></a>共享存储</h3><p>Pod持久化数据，专门存储到某个地方中</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114193124160.png"></p><p>使用 Volumn数据卷进行共享存储，案例如下所示</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114193341993.png"></p><h2 id="Pod镜像拉取策略"><a href="#Pod镜像拉取策略" class="headerlink" title="Pod镜像拉取策略"></a>Pod镜像拉取策略</h2><p>我们以具体实例来说，拉取策略就是 <code>imagePullPolicy</code></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114193605230.png"></p><p>拉取策略主要分为了以下几种</p><ul><li>IfNotPresent：默认值，镜像在宿主机上不存在才拉取</li><li>Always：每次创建Pod都会重新拉取一次镜像</li><li>Never：Pod永远不会主动拉取这个镜像</li></ul><h2 id="Pod资源限制"><a href="#Pod资源限制" class="headerlink" title="Pod资源限制"></a>Pod资源限制</h2><p>也就是我们Pod在进行调度的时候，可以对调度的资源进行限制，例如我们限制 Pod调度是使用的资源是 2C4G，那么在调度对应的node节点时，只会占用对应的资源，对于不满足资源的节点，将不会进行调度</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114194057920.png"></p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>我们在下面的地方进行资源的限制</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114194245517.png"></p><p>这里分了两个部分</p><ul><li>request：表示调度所需的资源</li><li>limits：表示最大所占用的资源</li></ul><h2 id="Pod重启机制"><a href="#Pod重启机制" class="headerlink" title="Pod重启机制"></a>Pod重启机制</h2><p>因为Pod中包含了很多个容器，假设某个容器初选问题了，那么就会触发Pod重启机制</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114194722125.png"></p><p>重启策略主要分为以下三种</p><ul><li>Always：当容器终止退出后，总是重启容器，默认策略 【nginx等，需要不断提供服务】</li><li>OnFailure：当容器异常退出（退出状态码非0）时，才重启容器。</li><li>Never：当容器终止退出，从不重启容器 【批量任务】</li></ul><h2 id="Pod健康检查"><a href="#Pod健康检查" class="headerlink" title="Pod健康检查"></a>Pod健康检查</h2><p>通过容器检查，原来我们使用下面的命令来检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod</span><br></pre></td></tr></table></figure><p>但是有的时候，程序可能出现了 <strong>Java</strong> 堆内存溢出，程序还在运行，但是不能对外提供服务了，这个时候就不能通过 容器检查来判断服务是否可用了</p><p>这个时候就可以使用应用层面的检查</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存活检查，如果检查失败，将杀死容器，根据Pod的restartPolicy【重启策略】来操作</span></span><br><span class="line">livenessProbe</span><br><span class="line"></span><br><span class="line"><span class="comment"># 就绪检查，如果检查失败，Kubernetes会把Pod从Service endpoints中剔除</span></span><br><span class="line">readinessProbe</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114195807564.png"></p><p>Probe支持以下三种检查方式</p><ul><li>http Get：发送HTTP请求，返回200 - 400 范围状态码为成功</li><li>exec：执行Shell命令返回状态码是0为成功</li><li>tcpSocket：发起TCP Socket建立成功</li></ul><h2 id="Pod调度策略"><a href="#Pod调度策略" class="headerlink" title="Pod调度策略"></a>Pod调度策略</h2><h3 id="创建Pod流程"><a href="#创建Pod流程" class="headerlink" title="创建Pod流程"></a>创建Pod流程</h3><ul><li>首先创建一个pod，然后创建一个API Server 和 Etcd【把创建出来的信息存储在etcd中】</li><li>然后创建 Scheduler，监控API Server是否有新的Pod，如果有的话，会通过调度算法，把pod调度某个node上</li><li>在node节点，会通过 <code>kubelet -- apiserver </code> 读取etcd 拿到分配在当前node节点上的pod，然后通过docker创建容器</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114201611308.png"></p><h3 id="影响Pod调度的属性"><a href="#影响Pod调度的属性" class="headerlink" title="影响Pod调度的属性"></a>影响Pod调度的属性</h3><p>Pod资源限制对Pod的调度会有影响</p><h4 id="根据request找到足够node节点进行调度"><a href="#根据request找到足够node节点进行调度" class="headerlink" title="根据request找到足够node节点进行调度"></a>根据request找到足够node节点进行调度</h4><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114194245517.png"></p><h4 id="节点选择器标签影响Pod调度"><a href="#节点选择器标签影响Pod调度" class="headerlink" title="节点选择器标签影响Pod调度"></a>节点选择器标签影响Pod调度</h4><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114202456151.png"></p><p>关于节点选择器，其实就是有两个环境，然后环境之间所用的资源配置不同</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114202643905.png"></p><p>我们可以通过以下命令，给我们的节点新增标签，然后节点选择器就会进行调度了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl label node node1 env_role=prod</span><br></pre></td></tr></table></figure><h4 id="节点亲和性"><a href="#节点亲和性" class="headerlink" title="节点亲和性"></a>节点亲和性</h4><p>节点亲和性 <strong>nodeAffinity</strong> 和 之前nodeSelector 基本一样的，根据节点上标签约束来决定Pod调度到哪些节点上</p><ul><li>硬亲和性：约束条件必须满足</li><li>软亲和性：尝试满足，不保证</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114203433939.png"></p><p>支持常用操作符：in、NotIn、Exists、Gt、Lt、DoesNotExists</p><p>反亲和性：就是和亲和性刚刚相反，如 NotIn、DoesNotExists等</p><h2 id="污点和污点容忍"><a href="#污点和污点容忍" class="headerlink" title="污点和污点容忍"></a>污点和污点容忍</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>nodeSelector 和 NodeAffinity，都是Prod调度到某些节点上，属于Pod的属性，是在调度的时候实现的。</p><p>Taint 污点：节点不做普通分配调度，是节点属性</p><h3 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h3><ul><li>专用节点【限制ip】</li><li>配置特定硬件的节点【固态硬盘】</li><li>基于Taint驱逐【在node1不放，在node2放】</li></ul><h3 id="查看污点情况"><a href="#查看污点情况" class="headerlink" title="查看污点情况"></a>查看污点情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node k8smaster | grep Taint</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114204124819.png"></p><p>污点值有三个</p><ul><li>NoSchedule：一定不被调度</li><li>PreferNoSchedule：尽量不被调度【也有被调度的几率】</li><li>NoExecute：不会调度，并且还会驱逐Node已有Pod</li></ul><h3 id="未节点添加污点"><a href="#未节点添加污点" class="headerlink" title="未节点添加污点"></a>未节点添加污点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node [node] key=value:污点的三个值</span><br></pre></td></tr></table></figure><p>举例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role=<span class="built_in">yes</span>:NoSchedule</span><br></pre></td></tr></table></figure><h3 id="删除污点"><a href="#删除污点" class="headerlink" title="删除污点"></a>删除污点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role:NoSchedule-</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114210022883.png"></p><h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h3><p>我们现在创建多个Pod，查看最后分配到Node上的情况</p><p>首先我们创建一个 nginx 的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment web --image=nginx</span><br></pre></td></tr></table></figure><p>然后使用命令查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114204917548.png"></p><p>我们可以非常明显的看到，这个Pod已经被分配到 k8snode1 节点上了</p><p>下面我们把pod复制5份，在查看情况pod情况</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment web --replicas=5</span><br></pre></td></tr></table></figure><p>我们可以发现，因为master节点存在污点的情况，所以节点都被分配到了 node1 和 node2节点上</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114205135282.png"></p><p>我们可以使用下面命令，把刚刚我们创建的pod都删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deployment web</span><br></pre></td></tr></table></figure><p>现在给了更好的演示污点的用法，我们现在给 node1节点打上污点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role=<span class="built_in">yes</span>:NoSchedule</span><br></pre></td></tr></table></figure><p>然后我们查看污点是否成功添加</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node k8snode1 | grep Taint</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114205516154.png"></p><p>然后我们在创建一个 pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建nginx pod</span></span><br><span class="line">kubectl create deployment web --image=nginx</span><br><span class="line"><span class="comment"># 复制五次</span></span><br><span class="line">kubectl scale deployment web --replicas=5</span><br></pre></td></tr></table></figure><p>然后我们在进行查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure><p>我们能够看到现在所有的pod都被分配到了 k8snode2上，因为刚刚我们给node1节点设置了污点</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114205654867.png"></p><p>最后我们可以删除刚刚添加的污点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node k8snode1 env_role:NoSchedule-</span><br></pre></td></tr></table></figure><h3 id="污点容忍"><a href="#污点容忍" class="headerlink" title="污点容忍"></a>污点容忍</h3><p>污点容忍就是某个节点可能被调度，也可能不被调度</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114210146123.png"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes集群YAML文件详解</title>
      <link href="/blog/2021/02/01/kubenetes/6/"/>
      <url>/blog/2021/02/01/kubenetes/6/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes集群YAML文件详解"><a href="#Kubernetes集群YAML文件详解" class="headerlink" title="Kubernetes集群YAML文件详解"></a>Kubernetes集群YAML文件详解</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>k8s 集群中对资源管理和资源对象编排部署都可以通过声明样式（YAML）文件来解决，也就是可以把需要对资源对象操作编辑到YAML 格式文件中，我们把这种文件叫做资源清单文件，通过kubectl 命令直接使用资源清单文件就可以实现对大量的资源对象进行编排部署了。一般在我们开发的时候，都是通过配置YAML文件来部署集群的。</p><p>YAML文件：就是资源清单文件，用于资源编排</p><h2 id="YAML文件介绍"><a href="#YAML文件介绍" class="headerlink" title="YAML文件介绍"></a>YAML文件介绍</h2><h3 id="YAML概述"><a href="#YAML概述" class="headerlink" title="YAML概述"></a>YAML概述</h3><p>YAML ：仍是一种标记语言。为了强调这种语言以数据做为中心，而不是以标记语言为重点。</p><p>YAML 是一个可读性高，用来表达数据序列的格式。</p><h3 id="YAML-基本语法"><a href="#YAML-基本语法" class="headerlink" title="YAML 基本语法"></a>YAML 基本语法</h3><ul><li>使用空格做为缩进</li><li>缩进的空格数目不重要，只要相同层级的元素左侧对齐即可</li><li>低版本缩进时不允许使用Tab 键，只允许使用空格</li><li>使用#标识注释，从这个字符一直到行尾，都会被解释器忽略</li><li>使用 — 表示新的yaml文件开始</li></ul><h3 id="YAML-支持的数据结构"><a href="#YAML-支持的数据结构" class="headerlink" title="YAML 支持的数据结构"></a>YAML 支持的数据结构</h3><h4 id="对象"><a href="#对象" class="headerlink" title="对象"></a>对象</h4><p>键值对的集合，又称为映射(mapping) &#x2F; 哈希（hashes） &#x2F; 字典（dictionary）</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对象类型：对象的一组键值对，使用冒号结构表示</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">Tom</span></span><br><span class="line"><span class="attr">age:</span> <span class="number">18</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yaml 也允许另一种写法，将所有键值对写成一个行内对象</span></span><br><span class="line"><span class="attr">hash:</span> &#123;<span class="attr">name:</span> <span class="string">Tom</span>, <span class="attr">age:</span> <span class="number">18</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数组类型：一组连词线开头的行，构成一个数组</span></span><br><span class="line">People</span><br><span class="line">- Tom</span><br><span class="line">- Jack</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数组也可以采用行内表示法</span></span><br><span class="line">People: [Tom, Jack]</span><br></pre></td></tr></table></figure><h2 id="YAML文件组成部分"><a href="#YAML文件组成部分" class="headerlink" title="YAML文件组成部分"></a>YAML文件组成部分</h2><p>主要分为了两部分，一个是控制器的定义 和 被控制的对象</p><h3 id="控制器的定义"><a href="#控制器的定义" class="headerlink" title="控制器的定义"></a>控制器的定义</h3><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114110444032.png"></p><h3 id="被控制的对象"><a href="#被控制的对象" class="headerlink" title="被控制的对象"></a>被控制的对象</h3><p>包含一些 镜像，版本、端口等</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114110600165.png"></p><h3 id="属性说明"><a href="#属性说明" class="headerlink" title="属性说明"></a>属性说明</h3><p>在一个YAML文件的控制器定义中，有很多属性名称</p><table><thead><tr><th align="center">属性名称</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">apiVersion</td><td align="center">API版本</td></tr><tr><td align="center">kind</td><td align="center">资源类型</td></tr><tr><td align="center">metadata</td><td align="center">资源元数据</td></tr><tr><td align="center">spec</td><td align="center">资源规格</td></tr><tr><td align="center">replicas</td><td align="center">副本数量</td></tr><tr><td align="center">selector</td><td align="center">标签选择器</td></tr><tr><td align="center">template</td><td align="center">Pod模板</td></tr><tr><td align="center">metadata</td><td align="center">Pod元数据</td></tr><tr><td align="center">spec</td><td align="center">Pod规格</td></tr><tr><td align="center">containers</td><td align="center">容器配置</td></tr></tbody></table><h2 id="如何快速编写YAML文件"><a href="#如何快速编写YAML文件" class="headerlink" title="如何快速编写YAML文件"></a>如何快速编写YAML文件</h2><p>一般来说，我们很少自己手写YAML文件，因为这里面涉及到了很多内容，我们一般都会借助工具来创建</p><h3 id="使用kubectl-create命令"><a href="#使用kubectl-create命令" class="headerlink" title="使用kubectl create命令"></a>使用kubectl create命令</h3><p>这种方式一般用于资源没有部署的时候，我们可以直接创建一个YAML配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尝试运行,并不会真正的创建镜像</span></span><br><span class="line">kubectl create deployment web --image=nginx -o yaml --dry-run</span><br></pre></td></tr></table></figure><p>或者我们可以输出到一个文件中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment web --image=nginx -o yaml --dry-run &gt; hello.yaml</span><br></pre></td></tr></table></figure><p>然后我们就在文件中直接修改即可</p><h3 id="使用kubectl-get命令导出yaml文件"><a href="#使用kubectl-get命令导出yaml文件" class="headerlink" title="使用kubectl get命令导出yaml文件"></a>使用kubectl get命令导出yaml文件</h3><p>可以首先查看一个目前已经部署的镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114113115649.png"></p><p>然后我们导出 nginx的配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get deploy nginx -o=yaml --<span class="built_in">export</span> &gt; nginx.yaml</span><br></pre></td></tr></table></figure><p>然后会生成一个 <code>nginx.yaml</code> 的配置文件</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114184538797.png" alt="image-20201114184538797"></p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速了解 Kubernetes</title>
      <link href="/blog/2021/02/01/kubenetes/2/"/>
      <url>/blog/2021/02/01/kubenetes/2/</url>
      
        <content type="html"><![CDATA[<h1 id="快速了解-Kubernetes"><a href="#快速了解-Kubernetes" class="headerlink" title="快速了解 Kubernetes"></a>快速了解 Kubernetes</h1><p>Kubernetes 脱胎于 Google 的 Borg 系统，是一个功能强大的容器编排系统。Kubernetes 及其整个生态系统（工具、模块、插件等）均使用 Go 语言编写，从而构成一套面向 API、可高速运行的程序集合，这些程序文档精良、易于参与贡献或在其上构建应用程序。</p><p>每个开发、运维或感兴趣的读者都应熟悉它的一些核心概念，以便理解这个系统及其不同的功能，以及为什么几乎所有人都在使用它。</p><p>在继续之前，我想提一下 Kubernetes 的几个顶级朋友（或竞争对手）：ECS、Nomad 和 Mesos。ECS 是 AWS 自己的编排解决方案，最近它又推出了托管在 AWS 上的 Kubernetes 系统——EKS。两者都支持 FARGATE，让用户无须关心所运行的物理资源。</p><p>Kubernetes 毫无疑问是最大赢家，作为一个开源系统，三大主流云服务商都以托管的方式提供了这项功能。但是，它比其他几个产品都要来得复杂和混乱。Kubernetes 可以处理几乎任何类型的容器负载，也有很多技巧，但这并不意味着每个人都应该使用它。其他解决方案或许也能满足某些公司的要求，例如，完全部署在 AWS 上的互联网产品公司，使用 ECS 会比 Kubernetes 具有更佳的生产环境体验，是的，也好于 EKS。</p><p>话虽如此，Kubernetes 的魔力在于：它可以部署在任何地方、它拥有一个活跃的社区，有数百个核心开发人员及数千个生态系统开源贡献者。它运行快速、具有创新性、模块化且面向 API，是一个对构建插件或服务非常友好的系统。</p><h1 id="Kubernetes-的-11-个部分"><a href="#Kubernetes-的-11-个部分" class="headerlink" title="Kubernetes 的 11 个部分"></a>Kubernetes 的 11 个部分</h1><h2 id="1-Pod"><a href="#1-Pod" class="headerlink" title="1. Pod"></a>1. Pod</h2><p>Pod 是 Kubernetes 中最小的可互动单元。一个 Pod 可以由多个容器组成，这些容器共同部署在单个节点上形成一个单元。一个 Pod 具有一个 IP，该 IP 在其容器之间共享。</p><p>在微服务世界中，一个 Pod 可以是执行后台工作或服务请求的微服务的单个实例。</p><h2 id="2-Node（节点）"><a href="#2-Node（节点）" class="headerlink" title="2. Node（节点）"></a>2. Node（节点）</h2><p>Node 是机器。它们是 Kubernetes 用于部署 Pod 的“裸机”（或虚拟机）。Node 为 Kubernetes 提供可用的集群资源用于以保持数据、运行作业、维护工作负载、创建网络路由等。</p><h2 id="3-Label（标签）与-Annotation（注解）"><a href="#3-Label（标签）与-Annotation（注解）" class="headerlink" title="3. Label（标签）与 Annotation（注解）"></a>3. Label（标签）与 Annotation（注解）</h2><p>Label 是 Kubernetes 及其最终用户用于过滤系统中相似资源的方式，也是资源与资源相互“访问”或关联的粘合剂。比如说，为 Deployment 打开端口的 Service。不论是监控、日志、调试或是测试，任何 Kubernetes 资源都应打上标签以供后续查验。例如，给系统中所有 Worker Pod 打上标签：app&#x3D;worker，之后即可在 kubectl 或 Kubernetes API 中使用 –selector 字段对其进行选择。</p><p>Annotation 与 Label 非常相似，但通常用于以自由的字符串形式保存不同对象的元数据，例如“更改原因: 安全补丁升级”。</p><h2 id="4-Service-Discovery（服务发现）"><a href="#4-Service-Discovery（服务发现）" class="headerlink" title="4. Service Discovery（服务发现）"></a>4. Service Discovery（服务发现）</h2><p>作为编排系统，Kubernetes 控制着不同工作负载的众多资源，负责管理 Pod、作业及所有需要通信的物理资源的网络。为此，Kubernetes 使用了 ETCD。</p><p>ETCD 是 Kubernetes 的“内部”数据库，Master 通过它来获取所有资源的位置。Kubernetes 还为服务提供了实际的“服务发现”——所有 Pod 使用了一个自定义的 DNS 服务器，通过解析其他服务的名称以获取其 IP 地址和端口。它在 Kubernetes 集群中“开箱即用”，无须进行设置。</p><h2 id="5-ReplicaSet（副本集）"><a href="#5-ReplicaSet（副本集）" class="headerlink" title="5. ReplicaSet（副本集）"></a>5. ReplicaSet（副本集）</h2><p>虽然 Pod 是一个物理性的运行任务，但通常使用单个实例是不够的。为了冗余并处理负载，出于某种原因（比如“伸缩”）需要对 Pod 进行复制。为了实现负责扩展和复制的层，Kubernetes 使用了 ReplicaSet。这个层以副本的数量表示系统的期望状态，并在任意给定时刻保持该系统的当前状态。</p><p>这也是配置自动伸缩的所在，在系统高负载时创建额外的副本，并在不再需要这些资源来支撑所运行的工作负载时进行缩容。</p><h2 id="6-DaemonSet（守护进程集）"><a href="#6-DaemonSet（守护进程集）" class="headerlink" title="6. DaemonSet（守护进程集）"></a>6. DaemonSet（守护进程集）</h2><p>有时候，应用程序每个节点需要的实例不超过一个。比如 FileBeat这类日志收集器就是个很好的例子。为了从各个节点收集日志，其代理需要运行在所有节点上，但每个节点只需要一个实例。Kubernetes 的 DaemonSet 即可用于创建这样的工作负载。</p><h2 id="7-StatefulSet（有状态集）"><a href="#7-StatefulSet（有状态集）" class="headerlink" title="7. StatefulSet（有状态集）"></a>7. StatefulSet（有状态集）</h2><p>尽管多数微服务涉及的都是不可变的无状态应用程序，但也有例外。有状态的工作负载有赖于磁盘卷的可靠支持。虽然应用程序容器本身可以是不可变的，可以使用更新的版本或更健康的实例来替代，但是所有副本还是需要数据的持久化。StatefulSet 即是用于这类需要在整个生命周期内使用同一节点的应用程序的部署。</p><p>它还保留了它的“名称”：容器内的 hostname 以及整个集群中服务发现的名称。3 个 ZooKeeper 构成的 StatefulSet 可以被命名 zk-1、zk-2 及 zk-3，也可以扩展到更多的成员 zk-4、zk-5 等等…… StatefulSets 还负责管理 PersistentVolumeClaim（Pod 上连接的磁盘）。</p><h2 id="8-Job（任务）"><a href="#8-Job（任务）" class="headerlink" title="8. Job（任务）"></a>8. Job（任务）</h2><p>Kubernetes 核心团队考虑了大部分使用编排系统的应用程序。虽然多数应用程序要求持续运行以同时处理服务器请求（比如 Web 服务器），但有时还是需要生成一批作业并在其完成后进行清理。比如，一个迷你的无服务器环境。</p><p>为了在 Kubernetes 中实现这一点，可以使用 Job 资源。正如其名，Job 的工作是生成容器来完成特定的工作，并在成功完成时销毁。举个例子，一组 Worker 从待处理和存储的数据队列中读取作业。一旦队列空了，就不再需要这些 Worker 了，直到下个批次准备好。</p><h2 id="9-ConfigMap（配置映射）及-Secret（机密配置）"><a href="#9-ConfigMap（配置映射）及-Secret（机密配置）" class="headerlink" title="9. ConfigMap（配置映射）及 Secret（机密配置）"></a>9. ConfigMap（配置映射）及 Secret（机密配置）</h2><p>现代应用程序的一个关键概念是无环境，并可通过注入的环境变量进行配置。应用程序应与其位置完全无关。为了在 Kubernetes 中实现这个重要的概念，就有了 ConfigMap。实际上这是一个环境变量键值列表，它们会被传递给正在运行的工作负载以确定不同的运行时行为。在同样的范畴下，Secret 与正常的配置条目类似，只是会进行加密以防类似密钥、密码、证书等敏感信息的泄漏。</p><h2 id="10-Deployment（部署）"><a href="#10-Deployment（部署）" class="headerlink" title="10. Deployment（部署）"></a>10. Deployment（部署）</h2><p>一切看起来都很美好，Pod 可以正常运行，如果上层有 ReplicaSet，还可以根据负载进行伸缩。不过，大家蜂拥而来，为的是能用新版本快速替换应用程序。我们想小规模地进行构建、测试和发布，以缩短反馈周期。使用 Deployments 即可持续地部署新软件，这是一组描述特定运行工作负载新需求的元数据。举个例子，发布新版本、错误修复，甚至是回滚（这是 Kubernetes 的另一个内部选项）。</p><p>在 Kubernetes 中部署软件可使用 2 个主要策略：</p><ul><li>替换——正如其名，使用新需求替换全部负载，自然会强制停机。对于快速替换非生产环境的资源，这很有帮助。</li><li>滚动升级——通过监听两个特定配置慢慢地将容器替换成新的：</li></ul><p>MaxAvailable——设置在部署新版本时可用的工作负载比例（或具体数量），100% 表示“我有 2 个容器，在部署时要保持 2 个存活以服务请求”；<br>b. MaxSurge——设置在当前存活容器的基础上部署的工作负载比例（或数量），100% 表示“我有 X 个容器，部署另外 X 个容器，然后开始滚动移除旧容器”。</p><h2 id="11-Storage（存储）"><a href="#11-Storage（存储）" class="headerlink" title="11. Storage（存储）"></a>11. Storage（存储）</h2><p>Kubernetes 在存储之上添加了一层抽象。工作负载可以为不同任务请求特定存储，甚至可以管理超过 Pod 生命周期的持久化。</p><h1 id="概念性理解"><a href="#概念性理解" class="headerlink" title="概念性理解"></a>概念性理解</h1><p>Kubernetes（现在仍然）是根据一些指导原则进行设计和开发的，构建在系统里的每个功能、概念和想法都考虑了社区因素。此外，最终用户会被引导以某种方式使用该系统，但这不是强迫的；最佳实践也是公开的，但作为一个开源免费的系统，你完全可以根据自身需要进行操作。</p><p>面向 API——系统每个部分构建时通过优良的文档和可操作的 API 来实现可交互性。核心开发人员会确保最终用户可以进行更改、查询和更新，以免将其阻挡在外或有不想要的过滤器。</p><p>欢迎包装工具——作为前一点的衍生产品，Kubernetes 对在其 API 之上构建的工具和包装器表示欢迎。作为一个原始平台，Kubernetes 是以一个非常可定制的方式进行构建的，以便他人使用，并进一步开发用于不同用例的工具。有些工具已经变得非常有名并被广泛使用，比如 Spinnaker、Istio 等等。</p><p>声明性状态——鼓励用户在系统中使用声明性描述而非命令式描述。这意味着，系统的状态和组件最好被描述为在某种版本控制（如 Git）中管理的代码，以此避免手工修改造成的困扰。因此，Kubernetes 减少了灾难恢复的难度、更易于在团队之间分享并传递责任。</p>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes集群管理工具kubectl</title>
      <link href="/blog/2021/02/01/kubenetes/1/"/>
      <url>/blog/2021/02/01/kubenetes/1/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes集群管理工具kubectl"><a href="#Kubernetes集群管理工具kubectl" class="headerlink" title="Kubernetes集群管理工具kubectl"></a>Kubernetes集群管理工具kubectl</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>kubectl是Kubernetes集群的命令行工具，通过kubectl能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署</p><h2 id="命令格式"><a href="#命令格式" class="headerlink" title="命令格式"></a>命令格式</h2><p>命令格式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl [<span class="built_in">command</span>] [<span class="built_in">type</span>] [name] [flags]</span><br></pre></td></tr></table></figure><p>参数</p><ul><li>command：指定要对资源执行的操作，例如create、get、describe、delete</li><li>type：指定资源类型，资源类型是大小写敏感的，开发者能够以单数 、复数 和 缩略的形式</li></ul><p>例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod pod1</span><br><span class="line">kubectl get pods pod1</span><br><span class="line">kubectl get po pod1</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201114095544185.png"></p><ul><li>name：指定资源的名称，名称也是大小写敏感的，如果省略名称，则会显示所有的资源，例如</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><ul><li>flags：指定可选的参数，例如，可用 -s 或者 -server参数指定Kubernetes API server的地址和端口</li></ul><h2 id="常见命令"><a href="#常见命令" class="headerlink" title="常见命令"></a>常见命令</h2><h3 id="kubectl-help-获取更多信息"><a href="#kubectl-help-获取更多信息" class="headerlink" title="kubectl help 获取更多信息"></a>kubectl help 获取更多信息</h3><p>通过 help命令，能够获取帮助信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取kubectl的命令</span></span><br><span class="line">kubectl --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取某个命令的介绍和使用</span></span><br><span class="line">kubectl get --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h3 id="基础命令"><a href="#基础命令" class="headerlink" title="基础命令"></a>基础命令</h3><p>常见的基础命令</p><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">create</td><td align="center">通过文件名或标准输入创建资源</td></tr><tr><td align="center">expose</td><td align="center">将一个资源公开为一个新的Service</td></tr><tr><td align="center">run</td><td align="center">在集群中运行一个特定的镜像</td></tr><tr><td align="center">set</td><td align="center">在对象上设置特定的功能</td></tr><tr><td align="center">get</td><td align="center">显示一个或多个资源</td></tr><tr><td align="center">explain</td><td align="center">文档参考资料</td></tr><tr><td align="center">edit</td><td align="center">使用默认的编辑器编辑一个资源</td></tr><tr><td align="center">delete</td><td align="center">通过文件名，标准输入，资源名称或标签来删除资源</td></tr></tbody></table><h3 id="部署命令"><a href="#部署命令" class="headerlink" title="部署命令"></a>部署命令</h3><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">rollout</td><td align="center">管理资源的发布</td></tr><tr><td align="center">rolling-update</td><td align="center">对给定的复制控制器滚动更新</td></tr><tr><td align="center">scale</td><td align="center">扩容或缩容Pod数量，Deployment、ReplicaSet、RC或Job</td></tr><tr><td align="center">autoscale</td><td align="center">创建一个自动选择扩容或缩容并设置Pod数量</td></tr></tbody></table><h3 id="集群管理命令"><a href="#集群管理命令" class="headerlink" title="集群管理命令"></a>集群管理命令</h3><table><thead><tr><th>命令</th><th>介绍</th></tr></thead><tbody><tr><td>certificate</td><td>修改证书资源</td></tr><tr><td>cluster-info</td><td>显示集群信息</td></tr><tr><td>top</td><td>显示资源(CPU&#x2F;M)</td></tr><tr><td>cordon</td><td>标记节点不可调度</td></tr><tr><td>uncordon</td><td>标记节点可被调度</td></tr><tr><td>drain</td><td>驱逐节点上的应用，准备下线维护</td></tr><tr><td>taint</td><td>修改节点taint标记</td></tr><tr><td></td><td></td></tr></tbody></table><h3 id="故障和调试命令"><a href="#故障和调试命令" class="headerlink" title="故障和调试命令"></a>故障和调试命令</h3><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">describe</td><td align="center">显示特定资源或资源组的详细信息</td></tr><tr><td align="center">logs</td><td align="center">在一个Pod中打印一个容器日志，如果Pod只有一个容器，容器名称是可选的</td></tr><tr><td align="center">attach</td><td align="center">附加到一个运行的容器</td></tr><tr><td align="center">exec</td><td align="center">执行命令到容器</td></tr><tr><td align="center">port-forward</td><td align="center">转发一个或多个</td></tr><tr><td align="center">proxy</td><td align="center">运行一个proxy到Kubernetes API Server</td></tr><tr><td align="center">cp</td><td align="center">拷贝文件或目录到容器中</td></tr><tr><td align="center">auth</td><td align="center">检查授权</td></tr></tbody></table><h3 id="其它命令"><a href="#其它命令" class="headerlink" title="其它命令"></a>其它命令</h3><table><thead><tr><th align="center">命令</th><th align="center">介绍</th></tr></thead><tbody><tr><td align="center">apply</td><td align="center">通过文件名或标准输入对资源应用配置</td></tr><tr><td align="center">patch</td><td align="center">使用补丁修改、更新资源的字段</td></tr><tr><td align="center">replace</td><td align="center">通过文件名或标准输入替换一个资源</td></tr><tr><td align="center">convert</td><td align="center">不同的API版本之间转换配置文件</td></tr><tr><td align="center">label</td><td align="center">更新资源上的标签</td></tr><tr><td align="center">annotate</td><td align="center">更新资源上的注释</td></tr><tr><td align="center">completion</td><td align="center">用于实现kubectl工具自动补全</td></tr><tr><td align="center">api-versions</td><td align="center">打印受支持的API版本</td></tr><tr><td align="center">config</td><td align="center">修改kubeconfig文件（用于访问API，比如配置认证信息）</td></tr><tr><td align="center">help</td><td align="center">所有命令帮助</td></tr><tr><td align="center">plugin</td><td align="center">运行一个命令行插件</td></tr><tr><td align="center">version</td><td align="center">打印客户端和服务版本信息</td></tr></tbody></table><h3 id="目前使用的命令"><a href="#目前使用的命令" class="headerlink" title="目前使用的命令"></a>目前使用的命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个nginx镜像</span></span><br><span class="line">kubectl create deployment nginx --image=nginx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对外暴露端口</span></span><br><span class="line">kubectl expose deployment nginx --port=80 --<span class="built_in">type</span>=NodePort</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看资源</span></span><br><span class="line">kubectl get pod, svc</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes简介</title>
      <link href="/blog/2021/02/01/kubenetes/0/"/>
      <url>/blog/2021/02/01/kubenetes/0/</url>
      
        <content type="html"><![CDATA[<h1 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h1><p>中文官网：<a href="https://kubernetes.io/zh">https://kubernetes.io/zh</a></p><p>中文社区：<a href="https://www.kubernetes.org.cn/">https://www.kubernetes.org.cn/</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>K8S主要讲的就是Kubernetes，首先Kubernetes首字母为K，末尾为s，中间一共有8个字母，所以简称K8s</p><h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><ul><li>Linux操作系统</li><li>Docker</li></ul><h1 id="课程简介"><a href="#课程简介" class="headerlink" title="课程简介"></a>课程简介</h1><ul><li><p>K8s概念和架构</p></li><li><p>从零搭建K8s集群</p><ul><li>基于客户端工具kubeadm搭建（简单，最多半小时）</li><li>基于二进制包方式（能看到内部的架构）</li></ul></li><li><p>K8s核心概念</p><ul><li>Pod：K8s管理的最小单位级，是所有业务类型的基础</li><li>Controller：控制器，有状态，无状态，一次任务，定时任务，守护进程</li><li>Service Ingress：对外暴露端口</li><li>RBAC：安全机制，权限模型</li><li>Helm：下载机制</li><li>持久化存储</li></ul></li><li><p>搭建集群监控平台系统</p></li><li><p>从零搭建高可用K8s集群</p></li><li><p>在集群环境部署项目</p></li></ul><h1 id="K8S概念和特性"><a href="#K8S概念和特性" class="headerlink" title="K8S概念和特性"></a>K8S概念和特性</h1><h2 id="部署发展历程"><a href="#部署发展历程" class="headerlink" title="部署发展历程"></a>部署发展历程</h2><p>我们的项目部署也在经历下面的这样一个历程</p><blockquote><p>传统部署 -&gt; 虚拟化部署时代 -&gt; 容器部署时代</p></blockquote><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122104102715.png"></p><ul><li><strong>传统部署时代</strong>：早期，组织在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。例如，如果在物理服务器上运行多个应用程序，则可能会出现-一个应用程序占用大部分资源的情况，结果可能导致其他应用程序的性能下降。–种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展，并且组织维护许多物理服务器的成本很高。</li><li><strong>虚拟化部署时代</strong>：作为解决方案，引入了虚拟化功能，它允许您在单个物理服务器的CPU.上运行多个虚拟机（VM）。虚拟化功能允许应用程序在VM之间隔离，并提供安全级别，因为一一个应用程序的信息不能被另一应用程序自由地访问。因为虚拟化可以轻松地添加或更新应用程序、降低硬件成本等等，所以虚拟化可以更好地利用物理服务器中的资源，并可以实现更好的可伸缩性。每个VM是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。</li><li><strong>容器部署时代</strong>：容器类似于VM，但是它们具有轻量级的隔离属性，可以在应用程序之间共享操作系统<br>（OS），因此，容器被认为是轻量级的。容器与VM类似，具有自己的文件系统、CPU、内存、进程空间等。由于它们与基础架构分离，因此可以跨云和OS分发进行移植。</li></ul><p>容器因具有许多优势而变得流行起来。下面列出了容器的一些好处：</p><ul><li>敏捷应用程序的创建和部署：与使用VM镜像相比，提高了容器镜像创建的简便性和效率。</li><li>持续开发、集成和部署：通过简单的回滚（由于镜像不可变性），提供可靠且频繁的容器镜像构建和部署。</li><li>关注开发与运维的分离：在构建&#x2F;时而不是在部署时创建应用程序容器镜像，将应用程序与基础架构分离。</li><li>可观察性：不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。</li><li>跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。</li><li>云和操作系统分发的可移植性：可在Ubuntu、RHEL、RHEL、CoreOS、本地、Google Kubernetes Engine和其它任何其它地方运行。</li><li>以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行OS到使用逻辑资源在OS上运行应用程序。</li><li>松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理-而不是在一台大型单机上器体运行。</li><li>资源隔离：可预测的应用程序性能。</li></ul><h2 id="K8S概述"><a href="#K8S概述" class="headerlink" title="K8S概述"></a>K8S概述</h2><p>kubernetes，简称K8s，是用8 代替8 个字符“ubernete”而成的缩写。是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes 提供了应用部署，规划，更新，维护的一种机制。</p><p>Kubernetes 是一个轻便的和可扩展的开源平台，用于管理容器化应用和服务。通过Kubernetes 能够进行应用的自动化部署和扩缩容。在Kubernetes 中，会将组成应用的容器组合成一个逻辑单元以更易管理和发现。</p><p>Kubernetes 积累了作为Google 生产环境运行工作负载15 年的经验，并吸收了来自于社区的最佳想法和实践。</p><p>传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新&#x2F;回滚等操作，当然也可以通过创建虚拟机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。</p><p>新的方式是通过部署容器方式实现，每个容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源。相对于虚拟机，容器能快速部署，由于容器与底层设施、机器文件系统解耦的。</p><blockquote><p>总结：</p><ul><li>K8s是谷歌在2014年发布的容器化集群管理系统</li><li>使用k8s进行容器化应用部署</li><li>使用k8s利于应用扩展</li><li>k8s目标实施让部署容器化应用更加简洁和高效</li></ul></blockquote><h2 id="K8S功能"><a href="#K8S功能" class="headerlink" title="K8S功能"></a>K8S功能</h2><h3 id="自动装箱"><a href="#自动装箱" class="headerlink" title="自动装箱"></a>自动装箱</h3><p>基于容器对应用运行环境的资源配置要求自动部署应用容器</p><h3 id="自我修复-自愈能力"><a href="#自我修复-自愈能力" class="headerlink" title="自我修复(自愈能力)"></a>自我修复(自愈能力)</h3><p>当容器失败时，会对容器进行重启</p><p>当所部署的Node节点有问题时，会对容器进行重新部署和重新调度</p><p>当容器未通过监控检查时，会关闭此容器直到容器正常运行时，才会对外提供服务</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20200928101336750.png"></p><p>如果某个服务器上的应用不响应了，Kubernetes会自动在其它的地方创建一个</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122112241092.png"></p><h3 id="水平扩展"><a href="#水平扩展" class="headerlink" title="水平扩展"></a>水平扩展</h3><p>通过简单的命令、用户UI 界面或基于CPU 等资源使用情况，对应用容器进行规模扩大或规模剪裁</p><blockquote><p>当我们有大量的请求来临时，我们可以增加副本数量，从而达到水平扩展的效果</p></blockquote><p>当黄色应用过度忙碌，会来扩展一个应用</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122112301750.png"></p><h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>用户不需使用额外的服务发现机制，就能够基于Kubernetes 自身能力实现服务发现和负载均衡</p><blockquote><p>对外提供统一的入口，让它来做节点的调度和负载均衡， 相当于微服务里面的网关？</p></blockquote><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20200928101711968.png"></p><h3 id="滚动更新"><a href="#滚动更新" class="headerlink" title="滚动更新"></a>滚动更新</h3><p>可以根据应用的变化，对应用容器运行的应用，进行一次性或批量式更新</p><blockquote><p>添加应用的时候，不是加进去就马上可以进行使用，而是需要判断这个添加进去的应用是否能够正常使用</p></blockquote><h3 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h3><p>可以根据应用部署情况，对应用容器运行的应用，进行历史版本即时回退</p><blockquote><p>类似于Git中的回滚</p></blockquote><h3 id="密钥和配置管理"><a href="#密钥和配置管理" class="headerlink" title="密钥和配置管理"></a>密钥和配置管理</h3><p>在不需要重新构建镜像的情况下，可以部署和更新密钥和应用配置，类似热部署。</p><h3 id="存储编排"><a href="#存储编排" class="headerlink" title="存储编排"></a>存储编排</h3><p>自动实现存储系统挂载及应用，特别对有状态应用实现数据持久化非常重要</p><p>存储系统可以来自于本地目录、网络存储(NFS、Gluster、Ceph 等)、公共云存储服务</p><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>提供一次性任务，定时任务；满足批量数据处理和分析的场景</p><h1 id="K8S架构组件"><a href="#K8S架构组件" class="headerlink" title="K8S架构组件"></a>K8S架构组件</h1><h2 id="完整架构图"><a href="#完整架构图" class="headerlink" title="完整架构图"></a>完整架构图</h2><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20200928103059652.png"></p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20200928110124821.png"></p><h2 id="架构细节"><a href="#架构细节" class="headerlink" title="架构细节"></a>架构细节</h2><p>K8S架构主要包含两部分：Master（主控节点）和 node（工作节点）</p><p>master节点架构图</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122113057343.png"></p><p>Node节点架构图</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122155629990.png"></p><p>k8s 集群控制节点，对集群进行调度管理，接受集群外用户去集群操作请求；</p><ul><li><p><strong>master</strong>：主控节点</p><ul><li>API Server：集群统一入口，以restful风格进行操作，同时交给etcd存储<ul><li>提供认证、授权、访问控制、API注册和发现等机制</li></ul></li><li>scheduler：节点的调度，选择node节点应用部署</li><li>controller-manager：处理集群中常规后台任务，一个资源对应一个控制器</li><li>etcd：存储系统，用于保存集群中的相关数据</li></ul></li><li><p><strong>Work node</strong>：工作节点</p><ul><li>Kubelet：master派到node节点代表，管理本机容器<ul><li>一个集群中每个节点上运行的代理，它保证容器都运行在Pod中</li><li>负责维护容器的生命周期，同时也负责Volume(CSI) 和 网络(CNI)的管理</li></ul></li><li>kube-proxy：提供网络代理，负载均衡等操作</li></ul></li><li><p>容器运行环境【<strong>Container Runtime</strong>】</p><ul><li>容器运行环境是负责运行容器的软件</li><li>Kubernetes支持多个容器运行环境：Docker、containerd、cri-o、rktlet以及任何实现Kubernetes CRI (容器运行环境接口) 的软件。</li></ul></li><li><p>fluentd：是一个守护进程，它有助于提升 集群层面日志</p></li></ul><h1 id="K8S核心概念"><a href="#K8S核心概念" class="headerlink" title="K8S核心概念"></a>K8S核心概念</h1><h2 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h2><ul><li>Pod是K8s中最小的单元</li><li>一组容器的集合</li><li>共享网络【一个Pod中的所有容器共享同一网络】</li><li>生命周期是短暂的（服务器重启后，就找不到了）</li></ul><h2 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h2><ul><li>声明在Pod容器中可访问的文件目录</li><li>可以被挂载到Pod中一个或多个容器指定路径下</li><li>支持多种后端存储抽象【本地存储、分布式存储、云存储】</li></ul><h2 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h2><ul><li>确保预期的pod副本数量【ReplicaSet】</li><li>无状态应用部署【Depoltment】<ul><li>无状态就是指，不需要依赖于网络或者ip</li></ul></li><li>有状态应用部署【StatefulSet】<ul><li>有状态需要特定的条件</li></ul></li><li>确保所有的node运行同一个pod 【DaemonSet】</li><li>一次性任务和定时任务【Job和CronJob】</li></ul><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><ul><li>定义一组Pod副本数目，版本等</li><li>通过控制器【Controller】维持Pod数目【自动回复失败的Pod】</li><li>通过控制器以指定的策略控制版本【滚动升级、回滚等】</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122161601349.png"></p><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><ul><li>定义一组pod的访问规则</li><li>Pod的负载均衡，提供一个或多个Pod的稳定访问地址</li><li>支持多种方式【ClusterIP、NodePort、LoadBalancer】</li></ul><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122161132055.png"></p><p>可以用来组合pod，同时对外提供服务</p><h2 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h2><p>label：标签，用于对象资源查询，筛选</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122161713638.png"></p><h2 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h2><p>命名空间，逻辑隔离</p><ul><li>一个集群内部的逻辑隔离机制【鉴权、资源】</li><li>每个资源都属于一个namespace</li><li>同一个namespace所有资源不能重复</li><li>不同namespace可以资源名重复</li></ul><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><p>我们通过Kubernetes的API来操作整个集群</p><p>同时我们可以通过 kubectl 、ui、curl 最终发送 http + json&#x2F;yaml 方式的请求给API Server，然后控制整个K8S集群，K8S中所有的资源对象都可以采用 yaml 或 json 格式的文件定义或描述</p><p>如下：使用yaml部署一个nginx的pod</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122162612448.png"></p><h1 id="完整流程"><a href="#完整流程" class="headerlink" title="完整流程"></a>完整流程</h1><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201122163512535.png"></p><ul><li>通过Kubectl提交一个创建RC（Replication Controller）的请求，该请求通过APlserver写入etcd</li><li>此时Controller Manager通过API Server的监听资源变化的接口监听到此RC事件</li><li>分析之后，发现当前集群中还没有它所对应的Pod实例</li><li>于是根据RC里的Pod模板定义一个生成Pod对象，通过APIServer写入etcd</li><li>此事件被Scheduler发现，它立即执行执行一个复杂的调度流程，为这个新的Pod选定一个落户的Node，然后通过API Server讲这一结果写入etcd中</li><li>目标Node上运行的Kubelet进程通过APiserver监测到这个”新生的Pod.并按照它的定义，启动该Pod并任劳任怨地负责它的下半生，直到Pod的生命结束</li><li>随后，我们通过Kubectl提交一个新的映射到该Pod的Service的创建请求</li><li>ControllerManager通过Label标签查询到关联的Pod实例，然后生成Service的Endpoints信息，并通过APIServer写入到etod中，</li><li>接下来，所有Node上运行的Proxy进程通过APIServer查询并监听Service对象与其对应的Endponts信息，建立一个软件方式的负载均衡器来实现Service访问到后端Pod的流量转发功能</li></ul>]]></content>
      
      
      <categories>
          
          <category> kubenetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MinIO对象存储</title>
      <link href="/blog/2021/01/29/minio/0/"/>
      <url>/blog/2021/01/29/minio/0/</url>
      
        <content type="html"><![CDATA[<h1 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h1><p>官方网站：<a href="https://min.io/">https://min.io/</a></p><p>中文文档：<a href="https://docs.min.io/cn/">https://docs.min.io/cn/</a></p><h1 id="常见的对象存储方式对比"><a href="#常见的对象存储方式对比" class="headerlink" title="常见的对象存储方式对比"></a>常见的对象存储方式对比</h1><ul><li><p>直接将图片保存到服务的硬盘</p><ul><li>优点：开发便捷，成本低</li><li>缺点：扩容困难</li></ul></li><li><p>使用分布式文件系统进行存储</p><ul><li>优点：容易实现扩容</li><li>缺点：开发复杂度稍大（尤其是开发复杂的功能）</li></ul></li><li><p>使用nfs做存储</p><ul><li>优点：开发较为便捷</li><li>缺点：需要有一定的运维知识进行部署和维护</li></ul></li><li><p>使用第三方的存储服务</p><ul><li>优点：开发简单，拥有强大功能，免维护</li><li>缺点：付费</li></ul></li></ul><h1 id="为何不采取FastDFs进行文件存储"><a href="#为何不采取FastDFs进行文件存储" class="headerlink" title="为何不采取FastDFs进行文件存储"></a>为何不采取FastDFs进行文件存储</h1><p>我们前面使用分布式文件系统FastDFS简直不要太爽，但是有几个问题不知道大家发现没有</p><ul><li>第一个就是FastDFS没有一个完善的官方文档，各种第三方文档满天飞。</li><li>第二个就是创建容器比较麻烦，要创建存储服务与跟踪服务.</li><li>第三个就是安全性问题</li></ul><h1 id="对象存储MinIO"><a href="#对象存储MinIO" class="headerlink" title="对象存储MinIO"></a>对象存储MinIO</h1><p>MinIO是世界上最快的对象存储服务器，在标准硬件上，读写速度分别为183GB&#x2F;s 和 171GB&#x2F;s，对象存储可以作为主要存储层，用于Spark，Presto，TensorFlow，H20.ai 以及替代产品等各种工作负载用于Hadoop HDFS</p><p>MinIO是一种高性能的分布式对象存储系统，它是软件定义的，可在行业标准硬件上运行，并且在Apache 2.0许可下，百分百开放源代码。</p><p>文档地址：<a href="https://docs.min.io/cn/">https://docs.min.io/cn/</a></p><h1 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h1><p>我们使用的是Docker的方式安装MinIO，首先拉取对应的镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull minio/minio</span><br></pre></td></tr></table></figure><p>然后我们需要创建两个目录，用于保存我们的文件和配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /home/minio/data</span><br><span class="line"><span class="built_in">mkdir</span> -p /home/minio/config</span><br></pre></td></tr></table></figure><h1 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h1><p>然后我们启动我们的容器，后面有个目录，就是我们需要挂载的硬盘目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 9090:9000 --name minio \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">-e <span class="string">&quot;MINIO_ACCESS_KEY=liuhz2021&quot;</span> \</span><br><span class="line">-e <span class="string">&quot;MINIO_SECRET_KEY=liuhz2021&quot;</span> \</span><br><span class="line">-v /home/minio/data:/data \</span><br><span class="line">-v /home/minio/config:/root/.minio \</span><br><span class="line">minio/minio server /data</span><br></pre></td></tr></table></figure><p>上面的配置中，包含两个重要的信息【以后登录时会用到，可以修改成自己的】</p><ul><li>MINIO_ACCESS_KEY：公钥</li><li>MINIO_SECRET_KEY：密钥</li></ul><p>运行成功后，我们就能看到我们下面的提示信息</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201015150408263.png"></p><p>如果需要后台运行，使用这条语句</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 9090:9000 --name minio \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">-e <span class="string">&quot;MINIO_ACCESS_KEY=liuhz2021&quot;</span> \</span><br><span class="line">-e <span class="string">&quot;MINIO_SECRET_KEY=liuhz2021&quot;</span> \</span><br><span class="line">-v /home/minio/data:/data \</span><br><span class="line">-v /home/minio/config:/root/.minio \</span><br><span class="line">minio/minio server /data</span><br></pre></td></tr></table></figure><h1 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h1><p>我是在VMware虚拟机（IP：192.168.243.149）搭建的，访问虚拟机就能够进入到我们的页面了【部署云服务器，需要自行开启安全组！】</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.243.149:9090</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201015150747476.png"></p><p>会有一个不错的登录页面，我们输入刚刚配置的账号和密码  liuhz2021  liuhz2021 即可进入</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201015150824563.png"></p><h1 id="创建bucket"><a href="#创建bucket" class="headerlink" title="创建bucket"></a>创建bucket</h1><p>我们首先需要创建一个桶，可以当成是一个目录，点击我们的右下角 加号 按钮，选择 create bucket进行创建</p><p>我们创建一个叫 liuhzblog 的桶，创建完成后，在侧边栏就能够看到我们刚刚创建的了</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201015151202753.png"></p><h1 id="上传文件"><a href="#上传文件" class="headerlink" title="上传文件"></a>上传文件</h1><p>然后我们选中我们的桶，在点击加号，选择 upload file进行文件上传</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201015151340507.png"></p><p>上传成功后，即可看到我们刚刚上传的文件列表了~</p><h1 id="SpringBoot整合MinIO"><a href="#SpringBoot整合MinIO" class="headerlink" title="SpringBoot整合MinIO"></a>SpringBoot整合MinIO</h1><p>参考文档：Java Client API文档</p><h2 id="修改权限"><a href="#修改权限" class="headerlink" title="修改权限"></a>修改权限</h2><p>如果要使用SDK，比如Java客户端来操作我们的minio的话，那么我们还需要修改一下我们的bucket权限</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201015151639447.png"></p><p>首先点击我们的liuhzblog的右边区域，点击Edit policy，然后添加我们的权限为 可读可写，保存即可</p><h2 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.minio<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>minio<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>8.0.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="添加application-properties"><a href="#添加application-properties" class="headerlink" title="添加application.properties"></a>添加application.properties</h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring.application.name</span>=<span class="string">minio-demo</span></span><br><span class="line"><span class="attr">server.port</span>=<span class="string">8080</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># MinIO</span></span><br><span class="line"><span class="attr">minio.endpoint</span>=<span class="string">http://192.168.243.149:9090</span></span><br><span class="line"><span class="attr">minio.accessKey</span>=<span class="string">liuhz2021</span></span><br><span class="line"><span class="attr">minio.secretKey</span>=<span class="string">liuhz2021</span></span><br><span class="line"><span class="attr">minio.bucketName</span>=<span class="string">liuhzblog</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spring.servlet.multipart.max-file-size</span>=<span class="string">50MB</span></span><br><span class="line"><span class="attr">spring.servlet.multipart.max-request-size</span>=<span class="string">100MB</span></span><br></pre></td></tr></table></figure><h2 id="添加配置文件"><a href="#添加配置文件" class="headerlink" title="添加配置文件"></a>添加配置文件</h2><p>然后我们需要编写配置文件，用于初始化配置 MinioClient装载到spring容器中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bonc.config;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io.minio.MinioClient;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Configuration;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> liuhz</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MinIoConfig</span> &#123;</span><br><span class="line">    <span class="meta">@Value(&quot;$&#123;minio.endpoint&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> String endpoint;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value(&quot;$&#123;minio.accessKey&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> String accessKey;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value(&quot;$&#123;minio.secretKey&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> String secretKey;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> MinioClient <span class="title function_">minioClient</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 使用MinIO服务的URL，端口，Access key和Secret key创建一个MinioClient对象</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">MinioClient</span>.Builder().endpoint(endpoint).credentials(accessKey, secretKey).build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="编写Controller"><a href="#编写Controller" class="headerlink" title="编写Controller"></a>编写Controller</h2><p>然后在写一个前端控制器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bonc.controller;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io.minio.GetPresignedObjectUrlArgs;</span><br><span class="line"><span class="keyword">import</span> io.minio.MinioClient;</span><br><span class="line"><span class="keyword">import</span> io.minio.PutObjectArgs;</span><br><span class="line"><span class="keyword">import</span> io.minio.http.Method;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Value;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.GetMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.PostMapping;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RequestParam;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.bind.annotation.RestController;</span><br><span class="line"><span class="keyword">import</span> org.springframework.web.multipart.MultipartFile;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> liuhz</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MinIoController</span> &#123;</span><br><span class="line"></span><br><span class="line">    MinioClient minioClient;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setMinioClient</span><span class="params">(MinioClient minioClient)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.minioClient = minioClient;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Value(&quot;$&#123;minio.bucketName&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">private</span> String bucketName;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@PostMapping(&quot;/upload&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">upload</span><span class="params">(<span class="meta">@RequestParam(&quot;file&quot;)</span> MultipartFile file)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        minioClient.putObject(</span><br><span class="line">                PutObjectArgs.builder()</span><br><span class="line">                        .bucket(bucketName)</span><br><span class="line">                        .object(file.getOriginalFilename())</span><br><span class="line">                        .stream(file.getInputStream(), file.getSize(), -<span class="number">1</span>)</span><br><span class="line">                        .contentType(file.getContentType())</span><br><span class="line">                        .build()</span><br><span class="line">        );</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;上传成功&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping(&quot;/download&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">download</span><span class="params">(<span class="meta">@RequestParam(&quot;fileName&quot;)</span> String fileName)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">return</span> minioClient.getPresignedObjectUrl(</span><br><span class="line">                GetPresignedObjectUrlArgs.builder()</span><br><span class="line">                        .bucket(bucketName)</span><br><span class="line">                        .object(fileName)</span><br><span class="line">                        .method(Method.GET)</span><br><span class="line">                        .expiry(<span class="number">60</span> * <span class="number">60</span> * <span class="number">24</span> * <span class="number">7</span>)</span><br><span class="line">                        .build()</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="测试图片上传"><a href="#测试图片上传" class="headerlink" title="测试图片上传"></a>测试图片上传</h2><p>下面我们就需要进行测试了，我们运行我们的项目，然后使用postman进行上传测试</p><p>首先我们在postman中添加我们的上传接口，然后在修改请求头中添加Content-Type</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Content-Type  multipart/form-data</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201018222200593.png"></p><p>然后在选择我们的图片上传</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20201018222251132.png"></p><p>最后在刷新MinIO，就能够看到我们刚刚上传的文件了</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210129222251132.png"></p><h2 id="获取图片URL"><a href="#获取图片URL" class="headerlink" title="获取图片URL"></a>获取图片URL</h2><p>获取到的URL具有过期时间，我们上面代码里设置的有效期是60 * 60 * 24 * 7，也就是7天</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210129222251133.png"></p><p>使用浏览器查看如下，在有效期内都是可以访问下载的</p><p><img src="https://gitee.com/xscorpion/ImageStore/raw/master/images/image-20210129222251134.png"></p>]]></content>
      
      
      <categories>
          
          <category> minio </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> minio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详解容器基本概念</title>
      <link href="/blog/2020/05/12/docker/1/"/>
      <url>/blog/2020/05/12/docker/1/</url>
      
        <content type="html"><![CDATA[<h1 id="详解容器基本概念"><a href="#详解容器基本概念" class="headerlink" title="详解容器基本概念"></a>详解容器基本概念</h1><h2 id="一、容器与镜像"><a href="#一、容器与镜像" class="headerlink" title="一、容器与镜像"></a>一、容器与镜像</h2><h3 id="什么是容器？"><a href="#什么是容器？" class="headerlink" title="什么是容器？"></a>什么是容器？</h3><p>在介绍容器的具体概念之前，先简单回顾一下操作系统是如何管理进程的。</p><p>首先，当我们登录到操作系统之后，可以通过 ps 等操作看到各式各样的进程，这些进程包括系统自带的服务和用户的应用进程。那么，这些进程都有什么样的特点？</p><ul><li>第一，这些进程可以相互看到、相互通信；</li><li>第二，它们使用的是同一个文件系统，可以对同一个文件进行读写操作；</li><li>第三，这些进程会使用相同的系统资源。</li></ul><p>这样的三个特点会带来什么问题呢？</p><ul><li>因为这些进程能够相互看到并且进行通信，高级权限的进程可以攻击其他进程；</li><li>因为它们使用的是同一个文件系统，因此会带来两个问题：这些进程可以对于已有的数据进行增删改查，具有高级权限的进程可能会将其他进程的数据删除掉，破坏掉其他进程的正常运行；此外，进程与进程之间的依赖可能会存在冲突，如此一来就会给运维带来很大的压力；</li><li>因为这些进程使用的是同一个宿主机的资源，应用之间可能会存在资源抢占的问题，当一个应用需要消耗大量 CPU 和内存资源的时候，就可能会破坏其他应用的运行，导致其他应用无法正常地提供服务。</li></ul><p>针对上述的三个问题，如何为进程提供一个独立的运行环境呢？</p><ul><li>针对不同进程使用同一个文件系统所造成的问题而言，Linux 和 Unix 操作系统可以通过 chroot 系统调用将子目录变成根目录，达到视图级别的隔离；进程在 chroot 的帮助下可以具有独立的文件系统，对于这样的文件系统进行增删改查不会影响到其他进程；</li><li>因为进程之间相互可见并且可以相互通信，使用 Namespace 技术来实现进程在资源的视图上进行隔离。在 chroot 和 Namespace 的帮助下，进程就能够运行在一个独立的环境下了；</li><li>但在独立的环境下，进程所使用的还是同一个操作系统的资源，一些进程可能会侵蚀掉整个系统的资源。为了减少进程彼此之间的影响，可以通过 Cgroup 来限制其资源使用率，设置其能够使用的 CPU 以及内存量。</li></ul><p>那么，应该如何定义这样的进程集合呢？</p><p>其实，<strong>容器就是一个视图隔离、资源可限制、独立文件系统的进程集合。</strong>所谓“视图隔离”就是能够看到部分进程以及具有独立的主机名等；控制资源使用率则是可以对于内存大小以及 CPU 使用个数等进行限制。容器就是一个进程集合，它将系统的其他资源隔离开来，具有自己独立的资源视图。</p><p>容器具有一个独立的文件系统，因为使用的是系统的资源，所以在独立的文件系统内不需要具备内核相关的代码或者工具，我们只需要提供容器所需的二进制文件、配置文件以及依赖即可。只要容器运行时所需的文件集合都能够具备，那么这个容器就能够运行起来。</p><h3 id="什么是镜像？"><a href="#什么是镜像？" class="headerlink" title="什么是镜像？"></a>什么是镜像？</h3><p>综上所述，我们将这些容器运行时所需要的所有的文件集合称之为容器镜像。</p><p>那么，一般都是通过什么样的方式来构建镜像的呢？通常情况下，我们会采用 Dockerfile 来构建镜像，这是因为 Dockerfile 提供了非常便利的语法糖，能够帮助我们很好地描述构建的每个步骤。当然，每个构建步骤都会对已有的文件系统进行操作，这样就会带来文件系统内容的变化，我们将这些变化称之为 changeset。当我们把构建步骤所产生的变化依次作用到一个空文件夹上，就能够得到一个完整的镜像。 changeset 的分层以及复用特点能够带来几点优势：</p><ul><li>第一，能够提高分发效率，简单试想一下，对于大的镜像而言，如果将其拆分成各个小块就能够提高镜像的分发效率，这是因为镜像拆分之后就可以并行下载这些数据；</li><li>第二，因为这些数据是相互共享的，也就意味着当本地存储上包含了一些数据的时候，只需要下载本地没有的数据即可，举个简单的例子就是 golang 镜像是基于 alpine 镜像进行构建的，当本地已经具有了 alpine 镜像之后，在下载 golang 镜像的时候只需要下载本地 alpine 镜像中没有的部分即可；</li><li>第三，因为镜像数据是共享的，因此可以节约大量的磁盘空间，简单设想一下，当本地存储具有了 alpine 镜像和 golang 镜像，在没有复用的能力之前，alpine 镜像具有 5M 大小，golang 镜像有 300M 大小，因此就会占用 305M 空间；而当具有了复用能力之后，只需要 300M 空间即可。</li></ul><h3 id="如何构建镜像？"><a href="#如何构建镜像？" class="headerlink" title="如何构建镜像？"></a>如何构建镜像？</h3><p>如下图所示的 Dockerfile 适用于描述如何构建 golang 应用的。</p><p>8 分钟入门 K8s | 详解容器基本概念<br>如图所示：</p><p><img src="https://pic2.zhimg.com/50/v2-a597994a026e8fb00975fa53cfabd86d_hd.jpg?source=1940ef5c"></p><p><img src="https://pic2.zhimg.com/80/v2-a597994a026e8fb00975fa53cfabd86d_720w.jpg?source=1940ef5c"></p><ol><li>FROM 行表示以下的构建步骤基于什么镜像进行构建，正如前面所提到的，镜像是可以复用的；</li><li>WORKDIR 行表示会把接下来的构建步骤都在哪一个相应的具体目录下进行，其起到的作用类似于 Shell 里面的 cd；</li><li>COPY 行表示的是可以将宿主机上的文件拷贝到容器镜像内；</li><li>RUN 行表示在具体的文件系统内执行相应的动作。当我们运行完毕之后就可以得到一个应用了；</li><li>CMD 行表示使用镜像时的默认程序名字。</li></ol><p>当有了 Dockerfile 之后，就可以通过 docker build 命令构建出所需要的应用。构建出的结果存储在本地，一般情况下，镜像构建会在打包机或者其他的隔离环境下完成。</p><p>那么，这些镜像如何运行在生产环境或者测试环境上呢？这时候就需要一个中转站或者中心存储，我们称之为 docker registry，也就是镜像仓库，其负责存储所有产生的镜像数据。我们只需要通过 docker push 就能够将本地镜像推动到镜像仓库中，这样一来，就能够在生产环境上或者测试环境上将相应的数据下载下来并运行了。</p><h3 id="如何运行容器？"><a href="#如何运行容器？" class="headerlink" title="如何运行容器？"></a>如何运行容器？</h3><p>运行一个容器一般情况下分为三步：</p><ul><li>第一步：从镜像仓库中将相应的镜像下载下来；</li><li>第二步：当镜像下载完成之后就可以通过 docker images 来查看本地镜像，这里会给出一个完整的列表，我们可以在列表中选中想要的镜像；</li><li>第三步：当选中镜像之后，就可以通过 docker run 来运行这个镜像得到想要的容器，当然可以通过多次运行得到多个容器。一个镜像就相当于是一个模板，一个容器就像是一个具体的运行实例，因此镜像就具有了一次构建、到处运行的特点。</li></ul><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>简单回顾一下，容器就是和系统其它部分隔离开来的进程集合，这里的其他部分包括进程、网络资源以及文件系统等。而镜像就是容器所需要的所有文件集合，其具备一次构建、到处运行的特点。</p><h2 id="二、容器的生命周期"><a href="#二、容器的生命周期" class="headerlink" title="二、容器的生命周期"></a>二、容器的生命周期</h2><h3 id="容器运行时的生命周期"><a href="#容器运行时的生命周期" class="headerlink" title="容器运行时的生命周期"></a>容器运行时的生命周期</h3><p>容器是一组具有隔离特性的进程集合，在使用 docker run 的时候会选择一个镜像来提供独立的文件系统并指定相应的运行程序。这里指定的运行程序称之为 initial 进程，这个 initial 进程启动的时候，容器也会随之启动，当 initial 进程退出的时候，容器也会随之退出。</p><p>因此，可以认为容器的生命周期和 initial 进程的生命周期是一致的。当然，因为容器内不只有这样的一个 initial 进程，initial 进程本身也可以产生其他的子进程或者通过 docker exec 产生出来的运维操作，也属于 initial 进程管理的范围内。当 initial 进程退出的时候，所有的子进程也会随之退出，这样也是为了防止资源的泄漏。 但是这样的做法也会存在一些问题，首先应用里面的程序往往是有状态的，其可能会产生一些重要的数据，当一个容器退出被删除之后，数据也就会丢失了，这对于应用方而言是不能接受的，所以需要将容器所产生出来的重要数据持久化下来。容器能够直接将数据持久化到指定的目录上，这个目录就称之为数据卷。</p><p>数据卷有一些特点，其中非常明显的就是数据卷的生命周期是独立于容器的生命周期的，也就是说容器的创建、运行、停止、删除等操作都和数据卷没有任何关系，因为它是一个特殊的目录，是用于帮助容器进行持久化的。简单而言，我们会将数据卷挂载到容器内，这样一来容器就能够将数据写入到相应的目录里面了，而且容器的退出并不会导致数据的丢失。</p><p>通常情况下，数据卷管理主要有两种方式：</p><ul><li>第一种是通过 bind 的方式，直接将宿主机的目录直接挂载到容器内；这种方式比较简单，但是会带来运维成本，因为其依赖于宿主机的目录，需要对于所有的宿主机进行统一管理。</li><li>第二种是将目录管理交给运行引擎。</li></ul><h2 id="三、容器项目架构"><a href="#三、容器项目架构" class="headerlink" title="三、容器项目架构"></a>三、容器项目架构</h2><h3 id="moby-容器引擎架构"><a href="#moby-容器引擎架构" class="headerlink" title="moby 容器引擎架构"></a>moby 容器引擎架构</h3><p>moby 是目前最流行的容器管理引擎，mobydaemon 会对上提供有关于容器、镜像、网络以及 Volume的管理。moby daemon 所依赖的最重要的组件就是 containerd，containerd 是一个容器运行时管理引擎，其独立于 moby daemon ，可以对上提供容器、镜像的相关管理。</p><p>containerd 底层有 containerd shim 模块，其类似于一个守护进程，这样设计的原因有几点：</p><ul><li>首先，containerd 需要管理容器生命周期，而容器可能是由不同的容器运行时所创建出来的，因此需要提供一个灵活的插件化管理。而 shim 就是针对于不同的容器运行时所开发的，这样就能够从 containerd 中脱离出来，通过插件的形式进行管理。</li><li>其次，因为 shim 插件化的实现，使其能够被 containerd 动态接管。如果不具备这样的能力，当 moby daemon 或者 containerd daemon 意外退出的时候，容器就没人管理了，那么它也会随之消失、退出，这样就会影响到应用的运行。</li><li>最后，因为随时可能会对 moby 或者 containerd 进行升级，如果不提供 shim 机制，那么就无法做到原地升级，也无法做到不影响业务的升级，因此 containerd shim 非常重要，它实现了动态接管的能力。</li></ul><p>本节课程只是针对于 moby 进行一个大致的介绍，在后续的课程也会详细介绍。</p><h2 id="四、容器-VS-VM"><a href="#四、容器-VS-VM" class="headerlink" title="四、容器 VS VM"></a>四、容器 VS VM</h2><h3 id="容器和-VM-之间的差异"><a href="#容器和-VM-之间的差异" class="headerlink" title="容器和 VM 之间的差异"></a>容器和 VM 之间的差异</h3><p>VM 利用 Hypervisor 虚拟化技术来模拟 CPU、内存等硬件资源，这样就可以在宿主机上建立一个 Guest OS，这是常说的安装一个虚拟机。</p><p>每一个 Guest OS 都有一个独立的内核，比如 Ubuntu、CentOS 甚至是 Windows 等，在这样的 Guest OS 之下，每个应用都是相互独立的，VM 可以提供一个更好的隔离效果。但这样的隔离效果需要付出一定的代价，因为需要把一部分的计算资源交给虚拟化，这样就很难充分利用现有的计算资源，并且每个 Guest OS 都需要占用大量的磁盘空间，比如 Windows 操作系统的安装需要 10<del>30G 的磁盘空间，Ubuntu 也需要 5</del>6G，同时这样的方式启动很慢。正是因为虚拟机技术的缺点，催生出了容器技术。 容器是针对于进程而言的，因此无需 Guest OS，只需要一个独立的文件系统提供其所需要文件集合即可。所有的文件隔离都是进程级别的，因此启动时间快于 VM，并且所需的磁盘空间也小于 VM。当然了，进程级别的隔离并没有想象中的那么好，隔离效果相比 VM 要差很多。</p><p>总体而言，容器和 VM 相比，各有优劣，因此容器技术也在向着强隔离方向发展。</p><h1 id="本文总结"><a href="#本文总结" class="headerlink" title="本文总结"></a>本文总结</h1><ul><li>容器是一个进程集合，具有自己独特的视图视角；</li><li>镜像是容器所需要的所有文件集合，其具备一次构建、到处运行的特点；</li><li>容器的生命周期和 initial 进程的生命周期是一样的；</li><li>容器和 VM 相比，各有优劣，容器技术在向着强隔离方向发展。</li></ul>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7安装Docker</title>
      <link href="/blog/2020/05/12/docker/0/"/>
      <url>/blog/2020/05/12/docker/0/</url>
      
        <content type="html"><![CDATA[<h1 id="CentOS-7-x86-64安装Docker"><a href="#CentOS-7-x86-64安装Docker" class="headerlink" title="CentOS-7-x86_64安装Docker"></a>CentOS-7-x86_64安装Docker</h1><p>要在CentOS上开始使用Docker Engine，请确保您满足先决条件，然后安装Docker。</p><h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><h3 id="操作系统要求"><a href="#操作系统要求" class="headerlink" title="操作系统要求"></a>操作系统要求</h3><p>要安装Docker Engine，您需要CentOS 7的维护版本。不支持或未测试存档版本。</p><p>必须启用centos-extras存储库。 该存储库默认情况下处于启用状态，但是如果已禁用它，则需要重新启用它。</p><p>建议使用overlay2存储驱动程序。</p><h3 id="卸载旧版本"><a href="#卸载旧版本" class="headerlink" title="卸载旧版本"></a>卸载旧版本</h3><p>较旧的Docker版本称为docker或docker-engine。 如果已安装这些程序，请卸载它们以及相关的依赖项。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-engine</span><br></pre></td></tr></table></figure><p>如果yum报告没有安装这些软件包，那就可以了。</p><p>&#x2F;var&#x2F;lib&#x2F;docker&#x2F;的内容（包括映像，容器，卷和网络）被保留。 Docker Engine软件包现在称为docker-ce。</p><h2 id="安装方式"><a href="#安装方式" class="headerlink" title="安装方式"></a>安装方式</h2><p>您可以根据需要以不同的方式安装Docker Engine：</p><ul><li>大多数用户会设置Docker的存储库并从中进行安装，以简化安装和升级任务。 这是推荐的方法。</li><li>一些用户下载并手动安装RPM软件包，并完全手动管理升级。 这在诸如在无法访问互联网的空白系统上安装Docker的情况下非常有用。</li><li>在测试和开发环境中，一些用户选择使用自动便利脚本来安装Docker。</li></ul><h3 id="使用存储库安装"><a href="#使用存储库安装" class="headerlink" title="使用存储库安装"></a>使用存储库安装</h3><p>在新主机上首次安装Docker Engine之前，需要设置Docker存储库。 之后，您可以从存储库安装和更新Docker。</p><h4 id="设置存储库"><a href="#设置存储库" class="headerlink" title="设置存储库"></a>设置存储库</h4><p>安装yum-utils软件包（提供yum-config-manager实用程序）并设置稳定的存储库。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install -y yum-utils</span><br><span class="line"></span><br><span class="line">$ sudo yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><h5 id="可选：启用每晚或测试存储库。"><a href="#可选：启用每晚或测试存储库。" class="headerlink" title="可选：启用每晚或测试存储库。"></a>可选：启用每晚或测试存储库。</h5><p>这些存储库包含在上面的docker.repo文件中，但默认情况下处于禁用状态。 您可以在稳定存储库旁边启用它们。 以下命令启用每晚存储库。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum-config-manager --enable docker-ce-nightly</span><br></pre></td></tr></table></figure><p>要启用测试通道，请运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum-config-manager --enable docker-ce-test</span><br></pre></td></tr></table></figure><p>您可以通过运行带有–disable标志的yum-config-manager命令来禁用夜间或测试存储库。 要重新启用它，请使用–enable标志。 以下命令禁用夜间存储库。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum-config-manager --disable docker-ce-nightly</span><br></pre></td></tr></table></figure><h4 id="安装DOCKER引擎"><a href="#安装DOCKER引擎" class="headerlink" title="安装DOCKER引擎"></a>安装DOCKER引擎</h4><p>1.安装最新版本的Docker Engine和容器，或者转到下一步安装特定版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure><p>如果提示您接受GPG密钥，请验证指纹是否与060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35匹配，如果是，则接受它。</p><h5 id="有多个Docker存储库？"><a href="#有多个Docker存储库？" class="headerlink" title="有多个Docker存储库？"></a>有多个Docker存储库？</h5><p>如果启用了多个Docker存储库，则在未在yum install或yum update命令中指定版本的情况下进行安装或更新，将始终安装可能的最高版本，这可能不适合您的稳定性需求。</p><p>2.要安装特定版本的Docker Engine，请在存储库中列出可用版本，然后选择并安装：</p><p>a. 列出并排序您存储库中可用的版本。 此示例按版本号（从高到低）对结果进行排序，并被截断：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ yum list docker-ce --showduplicates | sort -r</span><br><span class="line"></span><br><span class="line">docker-ce.x86_64  3:18.09.1-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64  3:18.09.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64  18.06.1.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64  18.06.0.ce-3.el7                    docker-ce-stable</span><br></pre></td></tr></table></figure><p>返回的列表取决于启用的存储库，并且特定于您的CentOS版本（在此示例中由.el7后缀指示）。</p><p>b.通过其完全合格的软件包名称安装特定版本，该软件包名称是软件包名称（docker-ce）加上版本字符串（第二列），从第一个冒号（:)开始，直至第一个连字符，并用连字符（-）分隔 ）。 例如docker-ce-18.09.1。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io</span><br></pre></td></tr></table></figure><p>3.启动Docker。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure><p>4.通过运行hello-world映像来验证Docker Engine是否已正确安装。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run hello-world</span><br></pre></td></tr></table></figure><h3 id="从软件包安装"><a href="#从软件包安装" class="headerlink" title="从软件包安装"></a>从软件包安装</h3><p>如果您无法使用Docker的存储库安装Docker，则可以下载适用于您的发行版的.rpm文件并手动安装。 每次要升级Docker Engine时，都需要下载一个新文件。</p><p>1.转到<a href="https://download.docker.com/linux/centos/">https://download.docker.com/linux/centos/</a> 并选择您的CentOS版本。然后浏览到x86_64&#x2F;stable&#x2F;Packages&#x2F;并下载要安装的Docker版本的.rpm文件。</p><p>注意：要安装每夜或测试（预发行版）软件包，请将上述URL中的稳定词改为每夜或测试。 了解每晚和测试频道。</p><p>2.安装Docker Engine，将下面的路径更改为您下载Docker软件包的路径。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install /path/to/package.rpm</span><br></pre></td></tr></table></figure><p>3.启动Docker。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure><p>4.通过运行hello-world映像来验证Docker Engine是否已正确安装。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run hello-world</span><br></pre></td></tr></table></figure><h3 id="使用便捷脚本进行安装"><a href="#使用便捷脚本进行安装" class="headerlink" title="使用便捷脚本进行安装"></a>使用便捷脚本进行安装</h3><p>Docker在get.docker.com和test.docker.com上提供了方便脚本，用于将Docker Engine-Community的边缘版本和测试版本快速且非交互地安装到开发环境中。 脚本的源代码位于docker-install存储库中。 不建议在生产环境中使用这些脚本，在使用它们之前，您应该了解潜在的风险：</p><ul><li>这些脚本需要root或sudo特权才能运行。 因此，在运行脚本之前，应仔细检查和审核脚本。</li><li>这些脚本会尝试检测Linux发行版和版本，并为您配置软件包管理系统。 此外，脚本不允许您自定义任何安装参数。 从Docker的角度或您自己组织的准则和标准的角度来看，这可能会导致配置不受支持。</li><li>这些脚本将安装软件包管理器的所有依赖项和建议，而无需进行确认。 这可能会安装大量软件包，具体取决于主机的当前配置。</li><li>该脚本未提供用于指定要安装哪个版本的Docker的选项，而是安装了在“ edge”通道中发布的最新版本。</li><li>如果已使用其他机制将Docker安装在主机上，请不要使用便捷脚本。</li></ul><p>本示例使用get.docker.com上的脚本在Linux上安装最新版本的Docker Engine-Community。 要安装最新的测试版本，请改用test.docker.com。 在下面的每个命令中，将每次出现的get替换为test。</p><p>警告：在本地运行之前，请务必检查从Internet下载的脚本。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl -fsSL https://get.docker.com -o get-docker.sh</span><br><span class="line">$ sudo sh get-docker.sh</span><br></pre></td></tr></table></figure><p>如果要使用Docker作为非root用户，则现在应考虑使用类似以下方式将用户添加到“ docker”组：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG docker your-user</span><br></pre></td></tr></table></figure><p>请记住注销并重新登录才能生效！</p><p>警告：将用户添加到“泊坞窗”组后，他们可以运行容器，该容器可用于获取Docker主机上的根特权。 有关更多信息，请参考Docker Daemon Attack Surface。</p><p>Docker Engine-社区已安装。 它会在基于DEB的发行版上自动启动。 在基于RPM的发行版上，您需要使用适当的systemctl或service命令手动启动它。 如该消息所示，默认情况下，非root用户不能运行Docker命令。</p><p>注意：要安装没有root特权的Docker，请参阅以非root用户身份运行Docker守护程序（无根模式）。无根模式目前可作为实验功能。</p><h3 id="卸载Docker-Engine"><a href="#卸载Docker-Engine" class="headerlink" title="卸载Docker Engine"></a>卸载Docker Engine</h3><p>1.卸载Docker Engine，CLI和Containerd软件包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum remove docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure><p>2主机上的映像，容器，卷或自定义配置文件不会自动删除。 要删除所有图像，容器和卷：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rm -rf /var/lib/docker</span><br></pre></td></tr></table></figure><p>您必须手动删除所有已编辑的配置文件。</p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> centos </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
